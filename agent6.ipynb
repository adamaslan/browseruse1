{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7487c83",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_google_genai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbrowser_use\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Configure logging\u001b[39;00m\n\u001b[32m      8\u001b[39m logging.basicConfig(level=logging.INFO)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_google_genai'"
     ]
    }
   ],
   "source": [
    "from browser_use import Agent, ChatGoogle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import hashlib\n",
    "\n",
    "# ===============================\n",
    "# Quick Setup with Immediate Feedback\n",
    "# ===============================\n",
    "\n",
    "print(\"üîß Initializing DrinksFoodLife Article Scraper...\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatGoogle(model=\"gemini-2.5-flash\")\n",
    "\n",
    "print(\"‚úÖ LLM initialized\")\n",
    "\n",
    "# ===============================\n",
    "# Configuration\n",
    "# ===============================\n",
    "\n",
    "OUTPUT_DIR = Path(\"drinksfoodlife_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "WEBSITE_URL = \"https://www.drinksfoodlife.com\"\n",
    "\n",
    "# ===============================\n",
    "# Helper Functions\n",
    "# ===============================\n",
    "\n",
    "def create_markdown_file(articles: List[Dict], filename: str) -> None:\n",
    "    \"\"\"Create a markdown file with article information.\"\"\"\n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# DrinksFoodLife.com Articles\\n\\n\")\n",
    "        f.write(f\"*Scraped on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\\n\")\n",
    "        f.write(f\"*Total Articles: {len(articles)}*\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "        \n",
    "        for idx, article in enumerate(articles, 1):\n",
    "            f.write(f\"## {idx}. {article.get('title', 'Untitled')}\\n\\n\")\n",
    "            f.write(f\"**URL:** [{article.get('url', 'N/A')}]({article.get('url', '#')})\\n\\n\")\n",
    "            f.write(f\"**Summary:**\\n\\n{article.get('summary', 'No summary available.')}\\n\\n\")\n",
    "            f.write(\"---\\n\\n\")\n",
    "    \n",
    "    logger.info(f\"üìÑ Markdown file created: {filepath}\")\n",
    "    print(f\"‚úÖ Markdown saved to: {filepath}\")\n",
    "\n",
    "def create_json_file(articles: List[Dict], filename: str) -> None:\n",
    "    \"\"\"Create a JSON file with article information.\"\"\"\n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    \n",
    "    output_data = {\n",
    "        \"scraped_at\": datetime.now().isoformat(),\n",
    "        \"source\": WEBSITE_URL,\n",
    "        \"total_articles\": len(articles),\n",
    "        \"articles\": articles\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    logger.info(f\"üìã JSON file created: {filepath}\")\n",
    "    print(f\"‚úÖ JSON saved to: {filepath}\")\n",
    "\n",
    "def generate_unique_filename(base_name: str, extension: str) -> str:\n",
    "    \"\"\"Generate a unique filename with timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    return f\"{base_name}_{timestamp}.{extension}\"\n",
    "\n",
    "# ===============================\n",
    "# Main Scraping Function\n",
    "# ===============================\n",
    "\n",
    "async def scrape_drinksfoodlife_articles() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape articles from DrinksFoodLife.com using browser-use agent.\n",
    "    Returns a list of article dictionaries with title, url, and summary.\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(\"üöÄ Starting article scraping from DrinksFoodLife.com...\")\n",
    "    print(\"üåê Opening browser and navigating to website...\")\n",
    "    \n",
    "    task = f\"\"\"\n",
    "    Navigate to {WEBSITE_URL} and scrape articles from the homepage.\n",
    "    \n",
    "    For each article you find:\n",
    "    1. Extract the exact article title\n",
    "    2. Extract the complete article URL\n",
    "    3. Click on the article to read the full content\n",
    "    4. Create a concise summary of approximately 100 tokens that captures the main points\n",
    "    5. Return to the homepage to continue finding more articles\n",
    "    \n",
    "    Try to collect at least 10-15 articles.\n",
    "    \n",
    "    Return the data as a JSON array with this exact structure:\n",
    "    [\n",
    "        {{\n",
    "            \"title\": \"Article Title\",\n",
    "            \"url\": \"https://www.drinksfoodlife.com/article-url\",\n",
    "            \"summary\": \"A 100-token summary of the article content...\"\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "    \n",
    "    Important:\n",
    "    - Make sure URLs are complete and clickable\n",
    "    - Summaries should be informative and capture key points\n",
    "    - Focus on recent/main articles visible on the homepage\n",
    "    \"\"\"\n",
    "    \n",
    "    agent = Agent(\n",
    "        task=task,\n",
    "        llm=llm,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = await agent.run()\n",
    "        logger.info(\"‚úÖ Scraping completed successfully\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error during scraping: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def parse_agent_result(result) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse the agent result to extract article data.\n",
    "    Handles various possible return formats.\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    \n",
    "    logger.info(\"üîç Parsing agent results...\")\n",
    "    \n",
    "    # If result is already a list\n",
    "    if isinstance(result, list):\n",
    "        articles = result\n",
    "    # If result has a specific method to get final output\n",
    "    elif hasattr(result, 'final_result'):\n",
    "        data = result.final_result()\n",
    "        if isinstance(data, list):\n",
    "            articles = data\n",
    "        elif isinstance(data, str):\n",
    "            try:\n",
    "                articles = json.loads(data)\n",
    "            except json.JSONDecodeError:\n",
    "                logger.warning(\"‚ö†Ô∏è Could not parse result as JSON\")\n",
    "    # If result is a string (try to parse as JSON)\n",
    "    elif isinstance(result, str):\n",
    "        try:\n",
    "            articles = json.loads(result)\n",
    "        except json.JSONDecodeError:\n",
    "            logger.warning(\"‚ö†Ô∏è Result is not valid JSON\")\n",
    "    \n",
    "    # Validate article structure\n",
    "    validated_articles = []\n",
    "    for article in articles:\n",
    "        if isinstance(article, dict) and 'title' in article and 'url' in article:\n",
    "            validated_articles.append({\n",
    "                'title': article.get('title', 'Untitled'),\n",
    "                'url': article.get('url', ''),\n",
    "                'summary': article.get('summary', 'No summary available.')\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"‚úÖ Successfully parsed {len(validated_articles)} articles\")\n",
    "    return validated_articles\n",
    "\n",
    "# ===============================\n",
    "# Main Execution\n",
    "# ===============================\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üçΩÔ∏è  DRINKSFOODLIFE.COM ARTICLE SCRAPER\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Scrape articles\n",
    "        result = await scrape_drinksfoodlife_articles()\n",
    "        \n",
    "        # Parse results\n",
    "        articles = parse_agent_result(result)\n",
    "        \n",
    "        if not articles:\n",
    "            logger.error(\"‚ùå No articles were extracted. Check the agent output.\")\n",
    "            logger.info(f\"Raw result type: {type(result)}\")\n",
    "            logger.info(f\"Raw result: {result}\")\n",
    "            print(\"\\n‚ö†Ô∏è  No articles found. The website structure may have changed.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n‚ú® Successfully extracted {len(articles)} articles!\\n\")\n",
    "        \n",
    "        # Generate unique filenames\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        md_filename = f\"drinksfoodlife_articles_{timestamp}.md\"\n",
    "        json_filename = f\"drinksfoodlife_articles_{timestamp}.json\"\n",
    "        \n",
    "        # Create output files\n",
    "        create_markdown_file(articles, md_filename)\n",
    "        create_json_file(articles, json_filename)\n",
    "        \n",
    "        # Summary\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üìä SCRAPING SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"‚úÖ Articles scraped: {len(articles)}\")\n",
    "        print(f\"‚è±Ô∏è  Time taken: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"üìÅ Output directory: {OUTPUT_DIR.absolute()}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        logger.info(\"‚ú® All done! Check the generated files.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Fatal error: {str(e)}\")\n",
    "        print(f\"\\n‚ùå An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "browz1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
