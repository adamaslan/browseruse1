{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc03efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [Agent] 🔗 Found URL in task: https://Investing.com, adding as initial action...\n",
      "🚀 Starting momentum investing research agent...\n",
      "INFO     [Agent] \u001b[34m🚀 Task: Search for the latest investor research and commentary on **momentum investing**, focusing on how strategies adapt to changing market dynamics. Prioritize high-quality sources including:\n",
      "- Investopedia (plain HTML, crawlable)\n",
      "- MarketWatch\n",
      "- Seeking Alpha (partially gated)\n",
      "- The Motley Fool\n",
      "- Yahoo Finance\n",
      "- Zacks\n",
      "- Morningstar (news + research)\n",
      "- InvestorPlace\n",
      "- Investing.com\n",
      "- Barron’s (abstracts crawlable)\n",
      "Tier 2 (Institutional Research):\n",
      "- CFA Institute Blog\n",
      "- SSRN (research papers)\n",
      "- NBER (working papers)\n",
      "- AQR Capital blogs\n",
      "- Research Affiliates insights\n",
      "\n",
      "Summarize findings into a **clean markdown report** with the following sections:\n",
      "## Key Findings\n",
      "## Implications for Investors\n",
      "## Practical Recommendations\n",
      "\n",
      "Also output a JSON array of structured takeaways with keys: [source, finding, implication, recommendation].\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m go_to_url: url: https://Investing.com, new_tab: False\n",
      "INFO     [tools] 🔗 Navigated to https://Investing.com\n",
      "INFO     [Agent] 🧠 Starting a browser-use version 0.7.3 with model=gemini-2.5-flash\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 1:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully loaded the initial URL, Investing.com. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Create a `todo.md` file to outline the task and then search for 'momentum investing' on Investing.com.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/3]\u001b[0m write_file: file_name: todo.md, content: # Momentum Investing Research\\n\\n## Goal: Search for the latest investor research...\n",
      "INFO     [tools] 💾 Data written to file todo.md successfully.\n",
      "INFO     [service] Something new appeared after action 1 / 3: actions input_text, click_element_by_index were not executed\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 2:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully created the todo.md file. The subsequent search actions were not executed due to a page change. Verdict: Partial Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Close the pop-up by clicking the 'x' icon (index 43) and then search for 'momentum investing' on Investing.com.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m click_element_by_index: index: 43, while_holding_ctrl: False\n",
      "INFO     [tools] 🖱️ Clicked element with index 43\n",
      "INFO     [auth] ────────────────────────────────────────\n",
      "INFO     [auth] 🌐  View the details of this run in Browser Use Cloud:\n",
      "INFO     [auth]     👉  https://cloud.browser-use.com/hotlink?user_code=7DRGCQHG3YJBDDW4\n",
      "INFO     [auth] ────────────────────────────────────────\n",
      "\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 3:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully closed the pop-up. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Input 'momentum investing' into the search bar (index 5) and then click the search button (index 6) on Investing.com.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/2]\u001b[0m input_text: index: 5, text: momentum investing, clear_existing: True\n",
      "INFO     [BrowserSession] ⌨️ Typed \"momentum investing\" into element with index 5\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 2/2]\u001b[0m click_element_by_index: index: 6, while_holding_ctrl: False\n",
      "INFO     [tools] 🖱️ Clicked element with index 6\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 4:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully input 'momentum investing' into the search bar and clicked the search button. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Scroll down the page to view the search results for 'momentum investing' on Investing.com.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m scroll: down: True, num_pages: 1.0, frame_element_index: None\n",
      "INFO     [tools] 🔍 Scrolled down the page by one page\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 5:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully scrolled down the page to view more search results. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Scroll down the page again to reveal more search results, then extract structured data from the page to gather information on momentum investing.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m scroll: down: True, num_pages: 1.0, frame_element_index: None\n",
      "INFO     [tools] 🔍 Scrolled down the page by one page\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 6:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully scrolled down the page to view more search results. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Extract structured data from the current page on Investing.com to gather information on momentum investing, focusing on how strategies adapt to changing market dynamics.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m extract_structured_data: query: latest investor research and commentary on momentum investing, focusing on how strategies ...\n",
      "INFO     [tools] 📄 <url>\n",
      "https://www.investing.com/\n",
      "</url>\n",
      "<query>\n",
      "latest investor research and commentary on momentum investing, focusing on how strategies adapt to changing market dynamics\n",
      "</query>\n",
      "<result>\n",
      "The webpage contains a \"Book Review: Dual Momentum Investing\" under its Analysis section.\n",
      "\n",
      "No other latest investor research or commentary on momentum investing, focusing on how strategies adapt to changing market dynamics, is available in the provided content.\n",
      "\n",
      "The content was truncated, use start_from_char=29992 to continue.\n",
      "</result>\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 7:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully extracted partial structured data, but the content was truncated. Verdict: Partial Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Continue extracting structured data from Investing.com using the `start_from_char` parameter, then mark the Investing.com search as complete in `todo.md`.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m extract_structured_data: query: latest investor research and commentary on momentum investing, focusing on how strategies ...\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 8:\n",
      "INFO     [Agent]   \u001b[31m⚠️ Eval: Failed to continue extracting structured data as the `start_from_char` value exceeded the content length, indicating no more content to extract. Verdict: Failure\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Mark the 'Search Investing.com for 'momentum investing'' task as complete in `todo.md` and then navigate to Investopedia to begin the next search.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m replace_file_str: file_name: todo.md, old_str: - [ ] Search Investing.com for momentum investing, new_str: - [x] Search Inv\n",
      "INFO     [tools] 💾 Successfully replaced all occurrences of \"- [ ] Search Investing.com for 'momentum investing'\" with \"- [x] Search Investing.com for 'momentum investing'\" in file todo.md\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 9:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully marked the Investing.com search as complete in todo.md. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Navigate to Investopedia to search for 'momentum investing'.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m go_to_url: url: https://www.investopedia.com/, new_tab: False\n",
      "INFO     [tools] 🔗 Navigated to https://www.investopedia.com/\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 10:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully navigated to Investopedia. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Click the search button (index 15) on Investopedia to reveal the search bar.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m click_element_by_index: index: 15, while_holding_ctrl: False\n",
      "INFO     [tools] 🖱️ Clicked element with index 15\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 11:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully clicked the search button, which revealed the search bar. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Input 'momentum investing' into the search bar (index 9) and then click the search button (index 10) on Investopedia.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/2]\u001b[0m input_text: index: 9, text: momentum investing, clear_existing: True\n",
      "INFO     [BrowserSession] ⌨️ Typed \"momentum investing\" into element with index 9\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 2/2]\u001b[0m click_element_by_index: index: 10, while_holding_ctrl: False\n",
      "INFO     [tools] 🖱️ Clicked element with index 10\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 12:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully input 'momentum investing' into the search bar and clicked the search button on Investopedia. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Scroll down the page to view more search results for 'momentum investing' on Investopedia, then extract structured data from the page.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m scroll: down: True, num_pages: 1.0, frame_element_index: None\n",
      "INFO     [tools] 🔍 Scrolled down the page by one page\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 13:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully scrolled down the page to view more search results. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Extract structured data from the current page on Investopedia to gather information on momentum investing, focusing on how strategies adapt to changing market dynamics, and then mark the Investopedia search as complete in `todo.md`.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m extract_structured_data: query: latest investor research and commentary on momentum investing, focusing on how strategies ...\n",
      "INFO     [tools] 📄 <url>\n",
      "https://www.investopedia.com/search?q=momentum+investing\n",
      "</url>\n",
      "<query>\n",
      "latest investor research and commentary on momentum investing, focusing on how strategies adapt to changing market dynamics\n",
      "</query>\n",
      "<result>\n",
      "The webpage content does not contain information on the latest investor research and commentary specifically on momentum investing, focusing on how strategies adapt to changing market dynamics.\n",
      "</result>\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 14:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully extracted structured data from Investopedia, but the content did not match the specific query. Verdict: Partial Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Mark the 'Search Investopedia for 'momentum investing'' task as complete in `todo.md` and then navigate to MarketWatch to begin the next search.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/2]\u001b[0m replace_file_str: file_name: todo.md, old_str: - [ ] Search Investopedia for momentum investing, new_str: - [x] Search Inve\n",
      "INFO     [tools] 💾 Successfully replaced all occurrences of \"- [ ] Search Investopedia for 'momentum investing'\" with \"- [x] Search Investopedia for 'momentum investing'\" in file todo.md\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 2/2]\u001b[0m go_to_url: url: https://www.marketwatch.com/, new_tab: False\n",
      "INFO     [tools] 🔗 Navigated to https://www.marketwatch.com/\n",
      "ERROR    [cdp_use.client] CDP Error for request 903: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "WARNING  [BrowserSession] CDP request ax_tree failed with exception: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [BrowserSession] Failed to build DOM tree without highlights: CDP requests failed or timed out: ax_tree\n",
      "WARNING  [BrowserSession] 🔍 DOMWatchdog.on_BrowserStateRequestEvent: DOM build failed: CDP requests failed or timed out: ax_tree, using minimal state\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 15:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully navigated to MarketWatch. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Wait for the MarketWatch page to fully load to reveal interactive elements, then locate the search bar or search icon to search for 'momentum investing'.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m wait: seconds: 3\n",
      "INFO     [tools] 🕒 waited for 0 seconds + 3 seconds for LLM call\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 16:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully waited for 3 seconds, allowing the MarketWatch page to load. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Click the 'Open Site Search' button (index 32) on MarketWatch to reveal the search bar, then input 'momentum investing' into the search bar and click the search button.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m click_element_by_index: index: 32, while_holding_ctrl: False\n",
      "INFO     [tools] 🖱️ Clicked element with index 32\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 17:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully clicked the 'Open Site Search' button. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Close the subscription pop-up on MarketWatch using the 'Escape' key, then locate and interact with the search bar to search for 'momentum investing'.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m send_keys: keys: Escape\n",
      "INFO     [BrowserSession] ⌨️ Sent keys: Escape\n",
      "INFO     [tools] ⌨️  Sent keys: Escape\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 18:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully closed the subscription pop-up on MarketWatch using the 'Escape' key. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Click the 'Open Site Search' button (index 32) on MarketWatch to reveal the search bar, then input 'momentum investing' into the search bar and click the search button.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m click_element_by_index: index: 32, while_holding_ctrl: False\n",
      "INFO     [tools] 🖱️ Clicked element with index 32\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 19:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully clicked the 'Open Site Search' button, but a new pop-up appeared, preventing further progress. Verdict: Partial Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Close the subscription pop-up on MarketWatch using the 'Escape' key, then locate and interact with the search bar to search for 'momentum investing'.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m send_keys: keys: Escape\n",
      "INFO     [BrowserSession] ⌨️ Sent keys: Escape\n",
      "INFO     [tools] ⌨️  Sent keys: Escape\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 20:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully closed the subscription pop-up on MarketWatch using the 'Escape' key. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Close the subscription pop-up on MarketWatch using the 'Escape' key, then locate and interact with the search bar to search for 'momentum investing'.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m send_keys: keys: Escape\n",
      "INFO     [BrowserSession] ⌨️ Sent keys: Escape\n",
      "INFO     [tools] ⌨️  Sent keys: Escape\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 21:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully closed the subscription pop-up on MarketWatch using the 'Escape' key, but it reappeared. Verdict: Failure\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Close the persistent subscription pop-up on MarketWatch by clicking the close button (index 151), then locate and interact with the search bar to search for 'momentum investing'.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m click_element_by_index: index: 151, while_holding_ctrl: False\n",
      "INFO     [tools] 🖱️ Clicked element with index 151\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 22:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully closed the persistent subscription pop-up on MarketWatch. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Click the 'Open Site Search' button (index 32) on MarketWatch to reveal the search bar, then input 'momentum investing' into the search bar and click the search button.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m click_element_by_index: index: 32, while_holding_ctrl: False\n",
      "INFO     [tools] 🖱️ Clicked element with index 32\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 23:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully clicked the 'Open Site Search' button, revealing the search bar. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Input 'momentum investing' into the search bar (index 34) and then click the search button (index 36) on MarketWatch.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/2]\u001b[0m input_text: index: 34, text: momentum investing, clear_existing: True\n",
      "INFO     [BrowserSession] ⌨️ Typed \"momentum investing\" into element with index 34\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 2/2]\u001b[0m click_element_by_index: index: 36, while_holding_ctrl: False\n",
      "INFO     [tools] 🖱️ Clicked element with index 36\n",
      "ERROR    [cdp_use.client] CDP Error for request 1560: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "WARNING  [BrowserSession] CDP request ax_tree failed with exception: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [BrowserSession] Failed to build DOM tree without highlights: CDP requests failed or timed out: ax_tree\n",
      "WARNING  [BrowserSession] 🔍 DOMWatchdog.on_BrowserStateRequestEvent: DOM build failed: CDP requests failed or timed out: ax_tree, using minimal state\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 24:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully input 'momentum investing' into the search bar and clicked the search button on MarketWatch. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Wait for the MarketWatch search results page to fully load, then scroll down to view more results and extract relevant information on momentum investing.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m wait: seconds: 3\n",
      "INFO     [tools] 🕒 waited for 0 seconds + 3 seconds for LLM call\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 25:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully waited for 3 seconds, allowing the MarketWatch search results page to load. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Scroll down the page to view more search results for 'momentum investing' on MarketWatch, then extract structured data from the page, and finally mark the MarketWatch search as complete in `todo.md`.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m scroll: down: True, num_pages: 3.0, frame_element_index: None\n",
      "INFO     [tools] 🔍 Scrolled down the page by 3.0 pages\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 26:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully scrolled down the page to view more search results on MarketWatch. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Extract structured data from the current page on MarketWatch to gather information on momentum investing, focusing on how strategies adapt to changing market dynamics, and then mark the MarketWatch search as complete in `todo.md`.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m extract_structured_data: query: latest investor research and commentary on momentum investing, focusing on how strategies ...\n",
      "INFO     [tools] 📄 <url>\n",
      "https://www.marketwatch.com/search?q=momentum%20investing&ts=0&tab=All%20News\n",
      "</url>\n",
      "<query>\n",
      "latest investor research and commentary on momentum investing, focusing on how strategies adapt to changing market dynamics\n",
      "</query>\n",
      "<result>\n",
      "The webpage content does not contain information about the latest investor research and commentary on momentum investing, or how these strategies adapt to changing market dynamics.\n",
      "</result>\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 27:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully extracted structured data from MarketWatch, but the content did not match the specific query. Verdict: Partial Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Mark the 'Search MarketWatch for 'momentum investing'' task as complete in `todo.md` and then navigate to Seeking Alpha to begin the next search.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/2]\u001b[0m replace_file_str: file_name: todo.md, old_str: - [ ] Search MarketWatch for momentum investing, new_str: - [x] Search Marke\n",
      "INFO     [tools] 💾 Successfully replaced all occurrences of \"- [ ] Search MarketWatch for 'momentum investing'\" with \"- [x] Search MarketWatch for 'momentum investing'\" in file todo.md\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 2/2]\u001b[0m go_to_url: url: https://seekingalpha.com/, new_tab: False\n",
      "INFO     [tools] 🔗 Navigated to https://seekingalpha.com/\n",
      "ERROR    [cdp_use.client] CDP Error for request 1810: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "WARNING  [BrowserSession] CDP request ax_tree failed with exception: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [BrowserSession] Failed to build DOM tree without highlights: CDP requests failed or timed out: ax_tree\n",
      "WARNING  [BrowserSession] 🔍 DOMWatchdog.on_BrowserStateRequestEvent: DOM build failed: CDP requests failed or timed out: ax_tree, using minimal state\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 28:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully navigated to Seeking Alpha. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Close the pop-up on Seeking Alpha using the 'Escape' key to access the main content and then locate the search bar to search for 'momentum investing'.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m send_keys: keys: Escape\n",
      "INFO     [BrowserSession] ⌨️ Sent keys: Escape\n",
      "INFO     [tools] ⌨️  Sent keys: Escape\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 29:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully closed the pop-up on Seeking Alpha using the 'Escape' key, but the pop-up reappeared or was not fully dismissed. Verdict: Failure\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Input 'momentum investing' into the search bar (index 42) and then click the search icon (index 43) on Seeking Alpha to search for relevant articles.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/2]\u001b[0m input_text: index: 42, text: momentum investing, clear_existing: True\n",
      "INFO     [BrowserSession] ⌨️ Typed \"momentum investing\" into element with index 42\n",
      "INFO     [service] Page changed after action: actions click_element_by_index are not yet executed\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 30:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Inputting 'momentum investing' into the search bar on Seeking Alpha was successful, but the subsequent click action was not executed due to a page change. Verdict: Partial Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Summarize the current findings and the incomplete status of the task, then terminate the process.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m text=\"The search for 'latest investor research and commentary on momentum investing, focusing on how strategies adapt to c...\n",
      "WARNING  [tools] Agent wanted to display files but none were found\n",
      "INFO     [Agent] \n",
      "📄 \u001b[31m Final Result:\u001b[0m \n",
      "The search for 'latest investor research and commentary on momentum investing, focusing on how strategies adapt to changing market dynamics' is not yet fully complete. I have attempted to search on Investing.com, Investopedia, MarketWatch, and Seeking Alpha.\n",
      "\n",
      "- **Investing.com**: Found a 'Book Review: Dual Momentum Investing' but no other specific research on how strategies adapt to changing market dynamics.\n",
      "- **Investopedia**: The search results did not contain information on the latest investor research and commentary specifically on momentum investing, focusing on how strategies adapt to changing market dynamics.\n",
      "- **MarketWatch**: The search results did not contain information about the latest investor research and commentary on momentum investing, or how these strategies adapt to changing market dynamics.\n",
      "- **Seeking Alpha**: I was able to input 'momentum investing' into the search bar, but the page changed before I could click the search button, indicating a persistent pop-up or dynamic content interference. The search on this site was not completed.\n",
      "\n",
      "I was unable to complete the search across all specified sources due to time constraints and persistent pop-ups on some websites (e.g., Seeking Alpha). Therefore, the task is incomplete.\n",
      "\n",
      "\n",
      "INFO     [Agent] ❌ Task completed without success\n",
      "❌ Error occurred: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  [cdp_use.client] WebSocket connection closed: no close frame received or sent\n"
     ]
    }
   ],
   "source": [
    "from browser_use import Agent, ChatGoogle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "# Load GOOGLE_API_KEY from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Gemini model\n",
    "llm = ChatGoogle(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# Optimized research task\n",
    "task = (\n",
    "    \"Search for the latest investor research and commentary on **momentum investing**, \"\n",
    "    \"focusing on how strategies adapt to changing market dynamics. \"\n",
    "    \"Prioritize high-quality sources including:\\n\"\n",
    "    \"- Investopedia (plain HTML, crawlable)\\n\"\n",
    "    \"- MarketWatch\\n\"\n",
    "    \"- Seeking Alpha (partially gated)\\n\"\n",
    "    \"- The Motley Fool\\n\"\n",
    "    \"- Yahoo Finance\\n\"\n",
    "    \"- Zacks\\n\"\n",
    "    \"- Morningstar (news + research)\\n\"\n",
    "    \"- InvestorPlace\\n\"\n",
    "    \"- Investing.com\\n\"\n",
    "    \"- Barron’s (abstracts crawlable)\\n\"\n",
    "    \"Tier 2 (Institutional Research):\\n\"\n",
    "    \"- CFA Institute Blog\\n\"\n",
    "    \"- SSRN (research papers)\\n\"\n",
    "    \"- NBER (working papers)\\n\"\n",
    "    \"- AQR Capital blogs\\n\"\n",
    "    \"- Research Affiliates insights\\n\\n\"\n",
    "    \"Summarize findings into a **clean markdown report** with the following sections:\\n\"\n",
    "    \"## Key Findings\\n\"\n",
    "    \"## Implications for Investors\\n\"\n",
    "    \"## Practical Recommendations\\n\\n\"\n",
    "    \"Also output a JSON array of structured takeaways with keys: \"\n",
    "    \"[source, finding, implication, recommendation].\"\n",
    ")\n",
    "\n",
    "# Create the browsing agent with browser configuration\n",
    "agent = Agent(\n",
    "    task=task,\n",
    "    llm=llm,\n",
    "    browser_config={\n",
    "        \"headless\": True,\n",
    "        \"browser_type\": \"chromium\",\n",
    "        \"browser_timeout\": 60,\n",
    "        \"viewport_size\": {\"width\": 1280, \"height\": 720},\n",
    "        \"extra_chromium_args\": [\n",
    "            \"--no-sandbox\",\n",
    "            \"--disable-dev-shm-usage\",\n",
    "            \"--disable-gpu\",\n",
    "            \"--disable-extensions\",\n",
    "            \"--disable-plugins\",\n",
    "            \"--disable-images\",       # Speed up loading\n",
    "            \"--disable-javascript\",   # Faster text-focused crawling\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        print(\"🚀 Starting momentum investing research agent...\")\n",
    "        history = await asyncio.wait_for(agent.run(max_steps=30), timeout=300)\n",
    "\n",
    "        # Save outputs\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        md_path = f\"momentum_investing_report_{timestamp}.md\"\n",
    "        summary_path = f\"momentum_investing_summary_{timestamp}.txt\"\n",
    "        structured_path = f\"momentum_investing_structured_{timestamp}.json\"\n",
    "\n",
    "        final_result = history.final_result() or \"\"\n",
    "\n",
    "        # --- Markdown Report ---\n",
    "        with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# Momentum Investing Research Report\\n\\n\")\n",
    "            f.write(f\"*Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\\n\")\n",
    "            f.write(final_result + \"\\n\")\n",
    "\n",
    "        # --- Plaintext Summary ---\n",
    "        with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"MOMENTUM INVESTING SUMMARY\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(final_result if final_result else \"No results generated by the agent.\")\n",
    "\n",
    "        # --- Structured JSON (if available in model output) ---\n",
    "        structured_data = []\n",
    "        if hasattr(history, \"structured_output\") and history.structured_output:\n",
    "            try:\n",
    "                structured_data = json.loads(history.structured_output)\n",
    "            except Exception:\n",
    "                structured_data = []\n",
    "\n",
    "        with open(structured_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(structured_data, f, indent=2)\n",
    "\n",
    "        print(f\"✅ Reports saved: {md_path}, {summary_path}, {structured_path}\")\n",
    "\n",
    "        # Show JSON preview\n",
    "        if structured_data:\n",
    "            print(\"📦 Structured JSON Preview:\")\n",
    "            print(json.dumps(structured_data[:3], indent=2))  # show first 3\n",
    "        else:\n",
    "            print(\"⚠️ No structured JSON detected in output. Only Markdown/summary saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error occurred: {str(e)}\")\n",
    "\n",
    "# Run\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1dfb2e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ChatGoogle.__init__() got an unexpected keyword argument 'max_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mEnvironment validation passed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Initialize the Gemini model with enhanced configuration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m llm = \u001b[43mChatGoogle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.5-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Lower temperature for more consistent research output\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8192\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Ensure sufficient context for comprehensive reports\u001b[39;49;00m\n\u001b[32m     41\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Enhanced research task with better structure and validation requirements\u001b[39;00m\n\u001b[32m     44\u001b[39m task = (\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSearch for the latest investor research and commentary on **momentum investing**, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfocusing on how strategies adapt to changing market dynamics. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m5. Validate that at least 5 different sources are referenced\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     79\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: ChatGoogle.__init__() got an unexpected keyword argument 'max_tokens'"
     ]
    }
   ],
   "source": [
    "from browser_use import Agent, ChatGoogle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "import aiofiles\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('research_agent.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load GOOGLE_API_KEY from .env\n",
    "load_dotenv()\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"Validate required environment variables and dependencies.\"\"\"\n",
    "    if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found in environment variables\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(\"research_outputs\").mkdir(exist_ok=True)\n",
    "    logger.info(\"Environment validation passed\")\n",
    "\n",
    "# Initialize the Gemini model with enhanced configuration\n",
    "llm = ChatGoogle(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.1,  # Lower temperature for more consistent research output\n",
    "    max_tokens=8192   # Ensure sufficient context for comprehensive reports\n",
    ")\n",
    "\n",
    "# Enhanced research task with better structure and validation requirements\n",
    "task = (\n",
    "    \"Search for the latest investor research and commentary on **momentum investing**, \"\n",
    "    \"focusing on how strategies adapt to changing market dynamics. \"\n",
    "    \"Prioritize high-quality sources including:\\n\"\n",
    "    \"Primary Sources (Tier 1):\\n\"\n",
    "    \"- Investopedia (plain HTML, crawlable)\\n\"\n",
    "    \"- MarketWatch\\n\"\n",
    "    \"- Seeking Alpha (partially gated)\\n\"\n",
    "    \"- The Motley Fool\\n\"\n",
    "    \"- Yahoo Finance\\n\"\n",
    "    \"- Zacks\\n\"\n",
    "    \"- Morningstar (news + research)\\n\"\n",
    "    \"- InvestorPlace\\n\"\n",
    "    \"- Investing.com\\n\"\n",
    "    \"- Barron's (abstracts crawlable)\\n\"\n",
    "    \"Secondary Sources (Tier 2 - Institutional Research):\\n\"\n",
    "    \"- CFA Institute Blog\\n\"\n",
    "    \"- SSRN (research papers)\\n\"\n",
    "    \"- NBER (working papers)\\n\"\n",
    "    \"- AQR Capital blogs\\n\"\n",
    "    \"- Research Affiliates insights\\n\\n\"\n",
    "    \"MANDATORY OUTPUT REQUIREMENTS:\\n\"\n",
    "    \"1. Summarize findings into a **clean markdown report** with these exact sections:\\n\"\n",
    "    \"## Executive Summary\\n\"\n",
    "    \"## Key Findings\\n\"\n",
    "    \"## Market Dynamics and Trends\\n\"\n",
    "    \"## Implications for Investors\\n\"\n",
    "    \"## Practical Recommendations\\n\"\n",
    "    \"## Risk Considerations\\n\"\n",
    "    \"## Sources Consulted\\n\\n\"\n",
    "    \"2. Include publication dates and credibility assessment for each source\\n\"\n",
    "    \"3. Ensure minimum 1000 words in the report\\n\"\n",
    "    \"4. End with a JSON array named 'STRUCTURED_TAKEAWAYS' containing objects with keys: \"\n",
    "    \"[source, publication_date, finding, implication, recommendation, confidence_level]\"\n",
    "    \"5. Validate that at least 5 different sources are referenced\"\n",
    ")\n",
    "\n",
    "def create_enhanced_browser_config() -> Dict:\n",
    "    \"\"\"Create optimized browser configuration for research tasks.\"\"\"\n",
    "    return {\n",
    "        \"headless\": True,\n",
    "        \"browser_type\": \"chromium\", \n",
    "        \"browser_timeout\": 90,  # Increased timeout for complex pages\n",
    "        \"viewport_size\": {\"width\": 1280, \"height\": 720},\n",
    "        \"extra_chromium_args\": [\n",
    "            \"--no-sandbox\",\n",
    "            \"--disable-dev-shm-usage\", \n",
    "            \"--disable-gpu\",\n",
    "            \"--disable-extensions\",\n",
    "            \"--disable-plugins\",\n",
    "            \"--disable-images\",\n",
    "            \"--disable-background-timer-throttling\",\n",
    "            \"--disable-backgrounding-occluded-windows\",\n",
    "            \"--disable-renderer-backgrounding\",\n",
    "            \"--disable-features=TranslateUI\",\n",
    "            \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "        ],\n",
    "        \"max_input_tokens\": 4000,  # Handle longer page content\n",
    "    }\n",
    "\n",
    "def extract_json_from_text(text: str) -> Optional[List[Dict]]:\n",
    "    \"\"\"Extract JSON array from model output using multiple strategies.\"\"\"\n",
    "    try:\n",
    "        # Strategy 1: Look for STRUCTURED_TAKEAWAYS\n",
    "        pattern = r'STRUCTURED_TAKEAWAYS[\\'\"]?\\s*[:=]\\s*(\\[.*?\\])'\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            return json.loads(match.group(1))\n",
    "        \n",
    "        # Strategy 2: Find any JSON array in the text\n",
    "        json_pattern = r'\\[\\s*\\{.*?\\}\\s*\\]'\n",
    "        matches = re.findall(json_pattern, text, re.DOTALL)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                parsed = json.loads(match)\n",
    "                if len(parsed) > 0 and isinstance(parsed[0], dict):\n",
    "                    return parsed\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        # Strategy 3: Look for code blocks with JSON\n",
    "        code_block_pattern = r'```(?:json)?\\s*(\\[.*?\\])\\s*```'\n",
    "        match = re.search(code_block_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            return json.loads(match.group(1))\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"JSON extraction failed: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def validate_report_quality(content: str) -> Dict[str, bool]:\n",
    "    \"\"\"Validate the quality and completeness of the generated report.\"\"\"\n",
    "    required_sections = [\n",
    "        \"Executive Summary\", \"Key Findings\", \"Market Dynamics\", \n",
    "        \"Implications for Investors\", \"Practical Recommendations\",\n",
    "        \"Risk Considerations\", \"Sources Consulted\"\n",
    "    ]\n",
    "    \n",
    "    validation = {\n",
    "        \"has_required_sections\": all(section in content for section in required_sections),\n",
    "        \"meets_length_requirement\": len(content.split()) >= 800,  # Slightly lower threshold\n",
    "        \"has_structured_data\": \"STRUCTURED_TAKEAWAYS\" in content or bool(extract_json_from_text(content)),\n",
    "        \"has_source_citations\": len(re.findall(r'\\[.*?\\]|\\*.*?\\*|Source:', content)) >= 3\n",
    "    }\n",
    "    \n",
    "    validation[\"overall_quality\"] = sum(validation.values()) >= 3\n",
    "    return validation\n",
    "\n",
    "async def save_outputs_async(final_result: str, timestamp: str) -> Dict[str, str]:\n",
    "    \"\"\"Asynchronously save all output files with enhanced error handling.\"\"\"\n",
    "    output_dir = Path(\"research_outputs\")\n",
    "    files_created = {}\n",
    "    \n",
    "    try:\n",
    "        # File paths\n",
    "        md_path = output_dir / f\"momentum_investing_report_{timestamp}.md\"\n",
    "        summary_path = output_dir / f\"momentum_investing_summary_{timestamp}.txt\"\n",
    "        structured_path = output_dir / f\"momentum_investing_structured_{timestamp}.json\"\n",
    "        \n",
    "        # Enhanced markdown report\n",
    "        markdown_content = (\n",
    "            f\"# Momentum Investing Research Report\\n\\n\"\n",
    "            f\"*Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\"\n",
    "            f\"*Research Agent Version: Enhanced v1.1*\\n\\n\"\n",
    "            f\"---\\n\\n{final_result}\\n\\n\"\n",
    "            f\"---\\n*End of Report*\"\n",
    "        )\n",
    "        \n",
    "        # Save markdown report\n",
    "        async with aiofiles.open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            await f.write(markdown_content)\n",
    "        files_created[\"markdown\"] = str(md_path)\n",
    "        \n",
    "        # Save plaintext summary\n",
    "        summary_content = (\n",
    "            \"MOMENTUM INVESTING RESEARCH SUMMARY\\n\"\n",
    "            \"=\" * 60 + \"\\n\\n\"\n",
    "            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "            \"=\" * 60 + \"\\n\\n\"\n",
    "            f\"{final_result if final_result else 'No results generated by the agent.'}\"\n",
    "        )\n",
    "        \n",
    "        async with aiofiles.open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            await f.write(summary_content)\n",
    "        files_created[\"summary\"] = str(summary_path)\n",
    "        \n",
    "        # Extract and save structured JSON\n",
    "        structured_data = extract_json_from_text(final_result) or []\n",
    "        \n",
    "        # Add metadata to structured output\n",
    "        output_metadata = {\n",
    "            \"generated_at\": datetime.now().isoformat(),\n",
    "            \"agent_version\": \"enhanced_v1.1\",\n",
    "            \"total_takeaways\": len(structured_data),\n",
    "            \"takeaways\": structured_data\n",
    "        }\n",
    "        \n",
    "        async with aiofiles.open(structured_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            await f.write(json.dumps(output_metadata, indent=2, ensure_ascii=False))\n",
    "        files_created[\"structured\"] = str(structured_path)\n",
    "        \n",
    "        logger.info(f\"Successfully saved {len(files_created)} output files\")\n",
    "        return files_created\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving outputs: {e}\")\n",
    "        return {}\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution function with comprehensive error handling and validation.\"\"\"\n",
    "    try:\n",
    "        # Validate environment\n",
    "        validate_environment()\n",
    "        \n",
    "        print(\"🚀 Starting enhanced momentum investing research agent...\")\n",
    "        logger.info(\"Research agent starting\")\n",
    "        \n",
    "        # Create the enhanced browsing agent\n",
    "        agent = Agent(\n",
    "            task=task,\n",
    "            llm=llm,\n",
    "            browser_config=create_enhanced_browser_config()\n",
    "        )\n",
    "        \n",
    "        # Run the agent with extended parameters\n",
    "        print(\"🔍 Agent executing research task...\")\n",
    "        history = await asyncio.wait_for(\n",
    "            agent.run(max_steps=35),  # Increased max steps\n",
    "            timeout=450  # Extended timeout to 7.5 minutes\n",
    "        )\n",
    "        \n",
    "        final_result = history.final_result() or \"\"\n",
    "        \n",
    "        if not final_result:\n",
    "            logger.warning(\"Agent returned empty result\")\n",
    "            print(\"⚠️ Agent completed but returned no results\")\n",
    "            return\n",
    "        \n",
    "        # Validate report quality\n",
    "        validation = validate_report_quality(final_result)\n",
    "        print(f\"📊 Report Quality Assessment: {validation}\")\n",
    "        \n",
    "        if not validation[\"overall_quality\"]:\n",
    "            logger.warning(\"Generated report may not meet quality standards\")\n",
    "            print(\"⚠️ Generated report may not meet all quality requirements\")\n",
    "        \n",
    "        # Save outputs asynchronously\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        files_created = await save_outputs_async(final_result, timestamp)\n",
    "        \n",
    "        if files_created:\n",
    "            print(\"✅ Reports successfully saved:\")\n",
    "            for file_type, path in files_created.items():\n",
    "                print(f\"   📄 {file_type.title()}: {path}\")\n",
    "        \n",
    "        # Enhanced JSON preview with error handling\n",
    "        structured_data = extract_json_from_text(final_result)\n",
    "        if structured_data:\n",
    "            print(f\"\\n📦 Structured Data Summary: {len(structured_data)} takeaways extracted\")\n",
    "            print(\"🔍 Sample takeaway:\")\n",
    "            sample = structured_data[0] if structured_data else {}\n",
    "            for key, value in sample.items():\n",
    "                print(f\"   {key}: {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}\")\n",
    "        else:\n",
    "            print(\"⚠️ No structured JSON detected in output\")\n",
    "            \n",
    "        # Final statistics\n",
    "        word_count = len(final_result.split())\n",
    "        print(f\"\\n📈 Research Statistics:\")\n",
    "        print(f\"   📝 Total words: {word_count:,}\")\n",
    "        print(f\"   ⏱️ Execution completed: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        logger.info(f\"Research completed successfully. Word count: {word_count}\")\n",
    "        \n",
    "    except asyncio.TimeoutError:\n",
    "        logger.error(\"Maximum search research agent timed out after 10 minutes\")\n",
    "        print(\"⏰ Research agent timed out after 10 minutes. The comprehensive search may require more time.\")\n",
    "        print(\"💡 Consider running during off-peak hours for better performance.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "        print(f\"❌ Error occurred: {str(e)}\")\n",
    "        print(\"💡 Check research_agent.log for detailed error information\")\n",
    "        print(\"🔄 Consider retrying with the run_with_retry() function\")\n",
    "\n",
    "# Enhanced execution with retry mechanism\n",
    "async def run_with_retry(max_retries: int = 2):\n",
    "    \"\"\"Run the main function with retry logic for transient failures.\"\"\"\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            await main()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                logger.warning(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n",
    "                print(f\"🔄 Attempt {attempt + 1} failed. Retrying in 10 seconds...\")\n",
    "                await asyncio.sleep(10)\n",
    "            else:\n",
    "                logger.error(f\"All {max_retries + 1} attempts failed\")\n",
    "                print(f\"❌ All retry attempts failed. Final error: {e}\")\n",
    "                raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run with retry mechanism\n",
    "    asyncio.run(run_with_retry())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c051be60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running research on: investopedia.com, marketwatch.com, seekingalpha.com\n",
      "INFO     [Agent] \u001b[34m🚀 Task: Search for the latest investor research and commentary on momentum investing, focusing on how strategies adapt to changing market dynamics. Summarize key takeaways, including insights on portfolio construction, trading costs, holding periods, scalability, and adaptation. Return a clean markdown summary with sections: Key Findings, Implications for Investors, and Practical Recommendations. Restrict yourself to these sources: investopedia.com, marketwatch.com, seekingalpha.com.\u001b[0m\n",
      "INFO     [Agent] 🧠 Starting a browser-use version 0.7.3 with model=gemini-2.5-flash\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 1:\n",
      "INFO     [Agent]   ❔ Eval: The previous goal was to start the task, and since this is the first step, there's no previous action to evaluate. Verdict: No previous action.\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Create a `todo.md` file with a detailed plan for searching the specified websites, extracting information, and compiling the final markdown summary.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m write_file: file_name: todo.md, content: # Momentum Investing Research and Commentary\\n\\n## Goal: Summarize key takeaways ...\n",
      "INFO     [tools] 💾 Data written to file todo.md successfully.\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 2:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully created the `todo.md` file. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Initialize the `results.md` file for storing extracted information and then mark this task as complete in `todo.md`.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/2]\u001b[0m write_file: file_name: results.md, content: # Momentum Investing Research Summary\\n\\n, append: False, trailing_newline: True\n",
      "INFO     [tools] 💾 Data written to file results.md successfully.\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 2/2]\u001b[0m replace_file_str: file_name: todo.md, old_str: - [ ] Initialize `results.md` for storing extracted information., new_str: - [\n",
      "INFO     [tools] 💾 Successfully replaced all occurrences of \"- [ ] Initialize `results.md` for storing extracted information.\" with \"- [x] Initialize `results.md` for storing extracted information.\" in file todo.md\n",
      "INFO     [auth] ────────────────────────────────────────\n",
      "INFO     [auth] 🌐  View the details of this run in Browser Use Cloud:\n",
      "INFO     [auth]     👉  https://cloud.browser-use.com/hotlink?user_code=EAQSLZ9XX7SPXGGV\n",
      "INFO     [auth] ────────────────────────────────────────\n",
      "\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 3:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully initialized the `results.md` file and marked the corresponding task in `todo.md` as complete. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Search Google for \"momentum investing strategies changing market dynamics investopedia.com\" to find relevant articles on Investopedia.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m search_google: query: momentum investing strategies changing market dynamics investopedia.com\n",
      "ERROR    [cdp_use.client] CDP Error for request 44: {'code': -32000, 'message': 'Not attached to an active page'}\n",
      "INFO     [tools] 🔍  Searched Google for 'momentum investing strategies changing market dynamics investopedia.com'\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 4:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully searched Google for 'momentum investing strategies changing market dynamics investopedia.com'. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Click on the 'Introduction to Momentum Trading' link on Investopedia.com to access the article.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m click_element_by_index: index: 33, while_holding_ctrl: False\n",
      "INFO     [tools] 🖱️ Clicked element with index 33\n",
      "ERROR    [cdp_use.client] CDP Error for request 117: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "WARNING  [BrowserSession] CDP request ax_tree failed with exception: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [BrowserSession] Failed to build DOM tree without highlights: CDP requests failed or timed out: ax_tree\n",
      "WARNING  [BrowserSession] 🔍 DOMWatchdog.on_BrowserStateRequestEvent: DOM build failed: CDP requests failed or timed out: ax_tree, using minimal state\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 5:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully navigated to the article page, but the content has not loaded yet. Verdict: Uncertain\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Wait for the page to fully load so that I can extract the relevant information.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m wait: seconds: 5\n",
      "INFO     [tools] 🕒 waited for 2 seconds + 3 seconds for LLM call\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 6:\n",
      "INFO     [Agent]   ❔ Eval: Waited for 5 seconds, but the page still appears to be loading. Verdict: Uncertain\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Scroll down the page by 10 pages to ensure all content is loaded and visible for extraction.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m scroll: down: True, num_pages: 10.0, frame_element_index: None\n",
      "INFO     [tools] 🔍 Scrolled down the page by 10.0 pages\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 7:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully scrolled down the page. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Extract structured data from the current Investopedia article, focusing on portfolio construction, trading costs, holding periods, scalability, and adaptation, and then update the `todo.md` file.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m extract_structured_data: query: Extract information about momentum investing strategies, focusing on portfolio constructio...\n",
      "INFO     [tools] 📄 Extracted content from https://www.investopedia.com/trading/introduction-to-momentum-trading/ for query: Extract information about momentum investing strategies, focusing on portfolio construction, trading costs, holding periods, scalability, and adaptation.\n",
      "Content saved to file system: Extracted content saved to file extracted_content_0.md successfully. and displayed in <read_state>.\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 8:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully extracted the first part of the content from the Investopedia article. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Extract the remaining content from the current Investopedia article using the provided `start_from_char` value.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m extract_structured_data: query: Extract information about momentum investing strategies, focusing on portfolio constructio...\n",
      "INFO     [tools] 📄 <url>\n",
      "https://www.investopedia.com/trading/introduction-to-momentum-trading/\n",
      "</url>\n",
      "<query>\n",
      "Extract information about momentum investing strategies, focusing on portfolio construction, trading costs, holding periods, scalability, and adaptation.\n",
      "</query>\n",
      "<result>\n",
      "The provided webpage content is truncated and does not contain information about momentum investing strategies, portfolio construction, trading costs, holding periods, scalability, or adaptation. To get more information, please use the `start_from_char` parameter to continue from where truncation occurred.\n",
      "</result>\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 9:\n",
      "INFO     [Agent]   \u001b[31m⚠️ Eval: Failed to extract the complete content from the Investopedia article using `extract_structured_data` even with `start_from_char`. Verdict: Failure\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Append the content of `extracted_content_0.md` to `results.md`, then mark the Investopedia extraction task as partially complete in `todo.md`, and then search for new articles on Investopedia or move to the next source.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m read_file: file_name: extracted_content_0.md\n",
      "INFO     [tools] 💾 Read from file extracted_content_0.md.\n",
      "<content>\n",
      "<url>\n",
      "https://www.investopedia.com/trading/introduction-to-momentum-trading/\n",
      "</url>\n",
      "<query>\n",
      "Extract information about momentum investing strategies, focusing on portfolio construction, trading costs, holding periods, scalability, and adaptation.\n",
      "</query>\n",
      "<result>\n",
      "**Momentum Investing Strategies**\n",
      "\n",
      "**Portfolio Construction:**\n",
      "*   **Security Selection:** Choose liquid and volatile individual securities, rather than mutual or exchange-traded funds (ETFs), with an average trading volume of at least 5 million shares per day. Avoid leveraged or inverse ETFs due to complex fund construction that can lead to inaccurate price tracking. Seek out \"story stocks\" in sectors like biotechs and small to midsize technology companies that capture public imagination and force analysts to recompute profit estimates.\n",
      "37 more lines...\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 10:\n",
      "INFO     [Agent]   \u001b[31m⚠️ Eval: Failed to extract the complete content from the Investopedia article using `extract_structured_data` even with `start_from_char`. Verdict: Failure\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Append the content of `extracted_content_0.md` to `results.md`, then mark the Investopedia extraction task as partially complete in `todo.md`, and then search for new articles on marketwatch.com.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/3]\u001b[0m write_file: file_name: results.md, content: **Momentum Investing Strategies**\\n\\n**Portfolio Construction:**\\n*   **Securi...\n",
      "INFO     [tools] 💾 Data appended to file results.md successfully.\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 2/3]\u001b[0m replace_file_str: file_name: todo.md, old_str: - [ ] Extract information from investopedia.com articles., new_str: - [x] Extr\n",
      "INFO     [tools] 💾 Successfully replaced all occurrences of \"- [ ] Extract information from investopedia.com articles.\" with \"- [x] Extract information from investopedia.com articles. (Partially complete)\" in file todo.md\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 3/3]\u001b[0m search_google: query: momentum investing strategies changing market dynamics marketwatch.com\n",
      "INFO     [tools] 🔍  Searched Google for 'momentum investing strategies changing market dynamics marketwatch.com'\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 11:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully appended the partial content from Investopedia to `results.md`, marked the Investopedia task as partially complete in `todo.md`, and searched Google for MarketWatch articles. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Click on the first MarketWatch article to begin extracting information about momentum investing strategies.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m click_element_by_index: index: 33, while_holding_ctrl: False\n",
      "INFO     [tools] 🖱️ Clicked element with index 33\n",
      "ERROR    [cdp_use.client] CDP Error for request 96: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "WARNING  [BrowserSession] CDP request ax_tree failed with exception: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [BrowserSession] Failed to build DOM tree without highlights: CDP requests failed or timed out: ax_tree\n",
      "WARNING  [BrowserSession] 🔍 DOMWatchdog.on_BrowserStateRequestEvent: DOM build failed: CDP requests failed or timed out: ax_tree, using minimal state\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 12:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully clicked on the MarketWatch article, but the page content has not loaded yet. Verdict: Uncertain\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Wait for the MarketWatch article page to fully load so that I can scroll and extract relevant information.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m wait: seconds: 5\n",
      "INFO     [tools] 🕒 waited for 2 seconds + 3 seconds for LLM call\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 13:\n",
      "INFO     [Agent]   ❔ Eval: Waited for 5 seconds, but the page still appears to be loading. Verdict: Uncertain\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Scroll down the MarketWatch article page by 10 pages to ensure all content is loaded and visible for extraction.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m scroll: down: True, num_pages: 10.0, frame_element_index: None\n",
      "INFO     [tools] 🔍 Scrolled down the page by 10.0 pages\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 14:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully scrolled down the page, but the relevant content is still not visible. Verdict: Failure\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Scroll up the MarketWatch article page by 10 pages to find the main article content for extraction.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m scroll: down: False, num_pages: 10.0, frame_element_index: None\n",
      "INFO     [tools] 🔍 Scrolled up the page by 10.0 pages\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 15:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully scrolled up the page, but the relevant content is still not visible. Verdict: Failure\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Scroll to the article title \"This momentum-stock fund can lower your risk. It has outperformed the S&P 500.\" to ensure I am at the beginning of the article and can begin extraction.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m scroll_to_text: text: This momentum-stock fund can lower your risk. It has outperformed the S&P 500.\n",
      "ERROR    [cdp_use.client] CDP Error for request 433: {'code': -32001, 'message': 'Session with given id not found.'}\n",
      "ERROR    [BrowserSession] \u001b[91m🚌 [DefaultActionWatchdog.on_ScrollToTextEvent(#8456)]    ❌ Failed (0.02s): RuntimeError: {'code': -32001, 'message': 'Session with given id not found.'}\u001b[0m\n",
      "ERROR    [bubus] ❌ EventBus_b89c6953🟢(⏳ 0 | ▶️ 0 | ✅ 50 ➡️ 28 👂) Error in event handler browser_use.browser.watchdog_base.DefaultActionWatchdog.on_ScrollToTextEvent(?▶ ScrollToTextEvent#8456 ✅) -> \n",
      "\u001b[91mRuntimeError({'code': -32001, 'message': 'Session with given id not found.'})\u001b[0m\n",
      "RuntimeError: {'code': -32001, 'message': 'Session with given id not found.'}\n",
      "\n",
      "INFO     [tools] Text 'This momentum-stock fund can lower your risk. It has outperformed the S&P 500.' not found or not visible on page\n",
      "ERROR    [cdp_use.client] CDP Error for request 430: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 431: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 432: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 433: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 434: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 435: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 436: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 437: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 438: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 439: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 440: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "WARNING  [BrowserSession] CDP request ax_tree failed with exception: {'code': -32602, 'message': 'Frame with the given frameId is not found.'}\n",
      "ERROR    [BrowserSession] Failed to build DOM tree without highlights: CDP requests failed or timed out: ax_tree\n",
      "WARNING  [BrowserSession] 🔍 DOMWatchdog.on_BrowserStateRequestEvent: DOM build failed: CDP requests failed or timed out: ax_tree, using minimal state\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 16:\n",
      "INFO     [Agent]   \u001b[31m⚠️ Eval: Failed to scroll to the article title, indicating the content is still not properly loaded or rendered. Verdict: Failure\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Wait for 5 seconds to allow the MarketWatch article page to fully load before attempting to extract information again.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m wait: seconds: 5\n",
      "INFO     [tools] 🕒 waited for 2 seconds + 3 seconds for LLM call\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 17:\n",
      "INFO     [Agent]   \u001b[31m⚠️ Eval: Waited for 5 seconds, but the MarketWatch page still appears to be loading and the content is not accessible. Verdict: Failure\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Close the current MarketWatch tab and then search for new articles on MarketWatch to find a working article, or move to the next source if MarketWatch continues to be problematic.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m close_tab: tab_id: 6C46\n",
      "INFO     [tools] 🗑️  Closed tab # 6C46 (marketwatch.com/story/…)\n",
      "ERROR    [cdp_use.client] CDP Error for request 562: {'code': -32602, 'message': 'No target with given id found'}\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 18:\n",
      "INFO     [Agent]   \u001b[32m👍 Eval: Successfully closed the problematic MarketWatch tab. Verdict: Success\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Search Google again for 'momentum investing strategies changing market dynamics marketwatch.com' to find a new article on MarketWatch, as the previous one was not loading correctly.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m search_google: query: momentum investing strategies changing market dynamics marketwatch.com\n",
      "ERROR    [cdp_use.client] CDP Error for request 563: {'code': -32001, 'message': 'Session with given id not found.'}\n",
      "ERROR    [BrowserSession] Navigation failed: RuntimeError: {'code': -32001, 'message': 'Session with given id not found.'}\n",
      "WARNING  [BrowserSession] [DownloadsWatchdog] No target info found for 1E0C83E30883A30277B5BAEAA5F96C46\n",
      "ERROR    [cdp_use.client] CDP Error for request 564: {'code': -32602, 'message': 'No target with given id found'}\n",
      "ERROR    [BrowserSession] \u001b[91m🚌 [BrowserSession.on_AgentFocusChangedEvent(#33b6)]      ❌ Failed (0.00s): RuntimeError: {'code': -32602, 'message': 'No target with given id found'}\u001b[0m\n",
      "ERROR    [cdp_use.client] CDP Error for request 565: {'code': -32602, 'message': 'No target with given id found'}\n",
      "ERROR    [BrowserSession] \u001b[91m🚌 [BrowserSession.on_AgentFocusChangedEvent(#33b6)]      ❌ CDP connected but failed to re-create CDP session after error \"RuntimeError: {'code': -32602, 'message': 'No target with given id found'}\" in \u001b[96mon_AgentFocusChangedEvent(AgentFocusChangedEvent#33b6)\u001b[0m: due to \u001b[91mRuntimeError: {'code': -32602, 'message': 'No target with given id found'}\u001b[0m\n",
      "\n",
      "ERROR    [bubus] ❌ EventBus_b89c6953🟢(⏳ 0 | ▶️ 1 | ✅ 49 ➡️ 28 👂) Error in event handler browser_use.browser.watchdog_base.BrowserSession.on_AgentFocusChangedEvent(?▶ AgentFocusChangedEvent#33b6 ✅) -> \n",
      "\u001b[91mRuntimeError({'code': -32602, 'message': 'No target with given id found'})\u001b[0m\n",
      "RuntimeError: {'code': -32602, 'message': 'No target with given id found'}\n",
      "\n",
      "ERROR    [BrowserSession] \u001b[91m🚌 [BrowserSession.on_NavigateToUrlEvent(#1e93)]          ❌ Failed (0.10s): RuntimeError: {'code': -32001, 'message': 'Session with given id not found.'}\u001b[0m\n",
      "ERROR    [cdp_use.client] CDP Error for request 566: {'code': -32602, 'message': 'No target with given id found'}\n",
      "ERROR    [BrowserSession] \u001b[91m🚌 [BrowserSession.on_NavigateToUrlEvent(#1e93)]          ❌ CDP connected but failed to re-create CDP session after error \"RuntimeError: {'code': -32001, 'message': 'Session with given id not found.'}\" in \u001b[96mon_NavigateToUrlEvent(NavigateToUrlEvent#1e93)\u001b[0m: due to \u001b[91mRuntimeError: {'code': -32602, 'message': 'No target with given id found'}\u001b[0m\n",
      "\n",
      "ERROR    [bubus] ❌ EventBus_b89c6953🟢(⏳ 0 | ▶️ 1 | ✅ 49 ➡️ 28 👂) Error in event handler browser_use.browser.watchdog_base.BrowserSession.on_NavigateToUrlEvent(?▶ NavigateToUrlEvent#1e93 🏃) -> \n",
      "\u001b[91mRuntimeError({'code': -32001, 'message': 'Session with given id not found.'})\u001b[0m\n",
      "RuntimeError: {'code': -32001, 'message': 'Session with given id not found.'}\n",
      "\n",
      "ERROR    [tools] Failed to search Google: {'code': -32001, 'message': 'Session with given id not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 567: {'code': -32602, 'message': 'No target with given id found'}\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 19:\n",
      "INFO     [Agent]   \u001b[31m⚠️ Eval: Failed to search Google for MarketWatch articles because the session was not found. Verdict: Failure\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Navigate to google.com to re-establish a search tab, then search for 'momentum investing strategies changing market dynamics marketwatch.com' to find new articles on MarketWatch.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m go_to_url: url: https://www.google.com, new_tab: True\n",
      "ERROR    [cdp_use.client] CDP Error for request 568: {'code': -32001, 'message': 'Session with given id not found.'}\n",
      "ERROR    [BrowserSession] Navigation failed: RuntimeError: {'code': -32001, 'message': 'Session with given id not found.'}\n",
      "WARNING  [BrowserSession] [DownloadsWatchdog] No target info found for 1E0C83E30883A30277B5BAEAA5F96C46\n",
      "ERROR    [cdp_use.client] CDP Error for request 569: {'code': -32602, 'message': 'No target with given id found'}\n",
      "ERROR    [BrowserSession] \u001b[91m🚌 [BrowserSession.on_AgentFocusChangedEvent(#5c9c)]      ❌ Failed (0.00s): RuntimeError: {'code': -32602, 'message': 'No target with given id found'}\u001b[0m\n",
      "ERROR    [cdp_use.client] CDP Error for request 570: {'code': -32602, 'message': 'No target with given id found'}\n",
      "ERROR    [BrowserSession] \u001b[91m🚌 [BrowserSession.on_AgentFocusChangedEvent(#5c9c)]      ❌ CDP connected but failed to re-create CDP session after error \"RuntimeError: {'code': -32602, 'message': 'No target with given id found'}\" in \u001b[96mon_AgentFocusChangedEvent(AgentFocusChangedEvent#5c9c)\u001b[0m: due to \u001b[91mRuntimeError: {'code': -32602, 'message': 'No target with given id found'}\u001b[0m\n",
      "\n",
      "ERROR    [bubus] ❌ EventBus_b89c6953🟢(⏳ 0 | ▶️ 1 | ✅ 49 ➡️ 28 👂) Error in event handler browser_use.browser.watchdog_base.BrowserSession.on_AgentFocusChangedEvent(?▶ AgentFocusChangedEvent#5c9c ✅) -> \n",
      "\u001b[91mRuntimeError({'code': -32602, 'message': 'No target with given id found'})\u001b[0m\n",
      "RuntimeError: {'code': -32602, 'message': 'No target with given id found'}\n",
      "\n",
      "ERROR    [BrowserSession] \u001b[91m🚌 [BrowserSession.on_NavigateToUrlEvent(#fc12)]          ❌ Failed (0.03s): RuntimeError: {'code': -32001, 'message': 'Session with given id not found.'}\u001b[0m\n",
      "ERROR    [cdp_use.client] CDP Error for request 571: {'code': -32602, 'message': 'No target with given id found'}\n",
      "ERROR    [BrowserSession] \u001b[91m🚌 [BrowserSession.on_NavigateToUrlEvent(#fc12)]          ❌ CDP connected but failed to re-create CDP session after error \"RuntimeError: {'code': -32001, 'message': 'Session with given id not found.'}\" in \u001b[96mon_NavigateToUrlEvent(NavigateToUrlEvent#fc12)\u001b[0m: due to \u001b[91mRuntimeError: {'code': -32602, 'message': 'No target with given id found'}\u001b[0m\n",
      "\n",
      "ERROR    [bubus] ❌ EventBus_b89c6953🟢(⏳ 0 | ▶️ 1 | ✅ 49 ➡️ 28 👂) Error in event handler browser_use.browser.watchdog_base.BrowserSession.on_NavigateToUrlEvent(?▶ NavigateToUrlEvent#fc12 🏃) -> \n",
      "\u001b[91mRuntimeError({'code': -32001, 'message': 'Session with given id not found.'})\u001b[0m\n",
      "RuntimeError: {'code': -32001, 'message': 'Session with given id not found.'}\n",
      "\n",
      "ERROR    [BrowserSession] ❌ Navigation failed: {'code': -32001, 'message': 'Session with given id not found.'}\n",
      "ERROR    [cdp_use.client] CDP Error for request 572: {'code': -32602, 'message': 'No target with given id found'}\n",
      "INFO     [Agent] \n",
      "\n",
      "INFO     [Agent] 📍 Step 20:\n",
      "INFO     [Agent]   \u001b[31m⚠️ Eval: Failed to navigate to google.com due to a lost browser session. Verdict: Failure\u001b[0m\n",
      "INFO     [Agent]   \u001b[34m🎯 Next goal: Terminate the task and provide the partial summary to the user, as I cannot continue due to a browser session error.\u001b[0m\n",
      "INFO     [Agent]   🦾 \u001b[34m[ACTION 1/1]\u001b[0m done: text: The task could not be fully completed due to a browser session error. I was able to extract some information f...\n",
      "INFO     [Agent] \n",
      "📄 \u001b[31m Final Result:\u001b[0m \n",
      "The task could not be fully completed due to a browser session error. I was able to extract some information from Investopedia regarding momentum investing strategies, focusing on portfolio construction. This information has been saved to `results.md`. I was unable to gather information from MarketWatch or Seeking Alpha. Please find the partial summary in the attached `results.md` file.\n",
      "\n",
      "Attachments:\n",
      "\n",
      "results.md:\n",
      "# Momentum Investing Research Summary\n",
      "\n",
      "\n",
      "\n",
      "**Momentum Investing Strategies**\n",
      "\n",
      "**Portfolio Construction:**\n",
      "*   **Security Selection:** Choose liquid and volatile individual securities, rather than mutual or exchange-traded funds (ETFs), with an average trading volume of at least 5 million shares per day. Avoid leveraged or inverse ETFs due to complex fund construction that can lead to inaccurate price tracking. Seek out \"story stocks\" in sectors like biotechs and small to midsize technology companies that capture public imagination and force analysts to recompute profit estimates.\n",
      "*   **Position Management:** Day trading with momentum strategies may require larger positions to compensate for the lack of greater profit potential from multi-day holds. Conversely, for multi-session holds, it is best to cut position size to allow for greater movement and stop placement further away from the present action.\n",
      "*   **Alternatives for Retail Investors:**\n",
      "    *   **Momentum ETFs:** These use rules-based approaches to select stocks exhibiting strong price momentum, automating the process. They typically rebalance holdings periodically (often quarterly) to maintain exposure to strong momentum characteristics. However, some momentum-focused products may be concentrated in specific sectors or styles, potentially increasing risk.\n",
      "    *   **Robo-Advisors:** Some platforms offer portfolio options that incorporate momentum strategies through algorithms.\n",
      "    *   **DIY Screening Tools:** Many financial and brokerage platforms provide tools to filter stocks based on momentum indicators.\n",
      "\n",
      "**Trading Costs:**\n",
      "*   Momentum investing involves high stock turnover, which can lead to increased transaction costs from fees and bid-ask spreads, eating into profits.\n",
      "*   High turnover can also result in more frequent capital gains distributions, impacting tax implications.\n",
      "*   These costs are a major concern, especially for rookie momentum traders, despite the availability of low-cost brokers.\n",
      "\n",
      "**Holding Periods:**\n",
      "*   Momentum investing involves taking short-term positions, buying rising securities, and selling them as soon as they show signs of going down.\n",
      "*   Skilled traders understand when to enter a position and how long to hold it, as risk increases with longer holding periods.\n",
      "*   The strategy aims for a shorter investment horizon, capitalizing on quick market movements.\n",
      "*   **Exit Strategy:** Exit when the price is rapidly moving into an overextended technical state (e.g., identified by vertical bars on a 60-minute chart or piercing the third or fourth standard deviation of a 20-day Bollinger Band). Tighten stops or consider a blind exit when technical barriers (like major trendlines or previous highs/lows) are hit. Take partial profits or exit when crossovers signal potential trend changes.\n",
      "*   Day trading is well-suited for momentum strategies, but multi-day holds are also possible with appropriate position sizing.\n",
      "*   Momentum ETFs typically rebalance their holdings periodically, often quarterly.\n",
      "*   Momentum trading specifically targets securities with strong recent performance over shorter time frames, differentiating it from trend following, which generally involves a longer-term approach.\n",
      "\n",
      "**Scalability:**\n",
      "*   Momentum investing is not practical for all investors and can lead to overall portfolio losses for individual investors. Professional traders at momentum funds often have an advantage due to quicker access to news and sophisticated analysis.\n",
      "*   The strategy requires frequent trading, sophisticated analysis, and constant market monitoring, which is time-consuming and potentially costly for the average investor.\n",
      "*   Alternatives like Momentum ETFs, robo-advisors, and DIY screening tools offer ways for retail investors to gain exposure without intensive personal management.\n",
      "\n",
      "**Adaptation:**\n",
      "*   **Risk Management:** Requires sophisticated risk management rules to address volatility, overcrowding, and hidden traps.\n",
      "*   **Discipline:** Traders must be disciplined to close trades at the first sign of weakness and immediately reallocate funds to a different asset showing strength.\n",
      "*   **Monitoring:** Momentum investors must monitor market details daily, if not hourly, to react to updates, changing trends, reversals, or unexpected news.\n",
      "*   **Market Conditions:** The strategy works best in a bull market where investor herding is more prevalent. In a bear market, the margin for profit shrinks due to increased investor caution.\n",
      "*   **Timing:** Early entry into a trend offers the greatest reward with the least risk; aging trends should be avoided.\n",
      "*   **Technical Indicators:** Tools like the relative strength index (RSI), moving average convergence divergence (MACD), and momentum oscillators aid in identifying overbought/oversold conditions, gauging trend strength, and signaling entry/exit points.\n",
      "*   **Automation:** Momentum trading can be automated using algorithmic trading systems that execute trades based on specific momentum indicators and pre-defined criteria. Automation allows for quicker execution and helps eliminate emotional biases, but algorithms require regular monitoring and adaptation to changing market conditions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INFO     [Agent] 👉 Attachment : /var/folders/49/6ydqkbq172ngzt6p49xfm6b00000gn/T/browser_use_agent_068bb88f-2316-7e9b-8000-24a1e73b6d26_1757120754/browseruse_agent_data/results.md\n",
      "INFO     [Agent] ❌ Task completed without success\n",
      "WARNING  [bubus] \n",
      "⚠️  WARNING: Total EventBus memory usage is 54.4MB (>50MB limit)\n",
      "Active EventBus instances: 4\n",
      "Memory breakdown:\n",
      "  - Agent_da17: 40.0MB (history=33, queue=0)\n",
      "  - Agent_6d26: 14.4MB (history=23, queue=0)\n",
      "Consider:\n",
      "  - Reducing max_history_size\n",
      "  - Clearing completed EventBus instances with stop(clear=True)\n",
      "  - Reducing event payload sizes\n",
      "\n",
      "WARNING  [cdp_use.client] WebSocket connection closed: no close frame received or sent\n",
      "WARNING  [bubus] \n",
      "⚠️  WARNING: Total EventBus memory usage is 54.4MB (>50MB limit)\n",
      "Active EventBus instances: 3\n",
      "Memory breakdown:\n",
      "  - Agent_da17: 40.0MB (history=33, queue=0)\n",
      "  - Agent_6d26: 14.4MB (history=23, queue=0)\n",
      "Consider:\n",
      "  - Reducing max_history_size\n",
      "  - Clearing completed EventBus instances with stop(clear=True)\n",
      "  - Reducing event payload sizes\n",
      "\n",
      "✅ Results from ['investopedia.com', 'marketwatch.com', 'seekingalpha.com'] saved.\n",
      "🔧 Manual research agent ready.\n",
      "➡️ Run: await run_research() inside Jupyter or asyncio.run(run_research()) in script\n"
     ]
    }
   ],
   "source": [
    "from browser_use import Agent, ChatGoogle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# ===============================\n",
    "# Setup\n",
    "# ===============================\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatGoogle(model=\"gemini-2.5-flash\")\n",
    "\n",
    "sources = [\n",
    "    \"investopedia.com\",\n",
    "    \"marketwatch.com\",\n",
    "    \"seekingalpha.com\",\n",
    "    \"fool.com\",\n",
    "    \"finance.yahoo.com\",\n",
    "    \"zacks.com\",\n",
    "    \"morningstar.com\",\n",
    "    \"investorplace.com\",\n",
    "    \"investing.com\",\n",
    "    \"barrons.com\",\n",
    "    \"cfainstitute.org\",\n",
    "    \"ssrn.com\",\n",
    "    \"nber.org\",\n",
    "    \"aqr.com\",\n",
    "    \"researchaffiliates.com\"\n",
    "]\n",
    "\n",
    "visited_sources_file = \"visited_sources.json\"\n",
    "markdown_path = \"momentum_investing_summary.md\"\n",
    "structured_path = \"momentum_investing_data.json\"\n",
    "\n",
    "# ===============================\n",
    "# Helpers\n",
    "# ===============================\n",
    "\n",
    "def load_visited_sources():\n",
    "    if os.path.exists(visited_sources_file):\n",
    "        with open(visited_sources_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return set(json.load(f))\n",
    "    return set()\n",
    "\n",
    "def save_visited_sources(visited):\n",
    "    with open(visited_sources_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(visited), f, indent=2)\n",
    "\n",
    "def get_remaining_sources():\n",
    "    visited = load_visited_sources()\n",
    "    return [s for s in sources if s not in visited]\n",
    "\n",
    "def append_markdown(new_text, path=markdown_path):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n---\\n\\n\")\n",
    "        f.write(new_text)\n",
    "\n",
    "def append_structured(new_data, path=structured_path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing = json.load(f)\n",
    "    else:\n",
    "        existing = []\n",
    "\n",
    "    if isinstance(new_data, str):\n",
    "        try:\n",
    "            new_data = json.loads(new_data)\n",
    "        except:\n",
    "            new_data = [{\"raw\": new_data}]\n",
    "\n",
    "    combined = existing + new_data\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined, f, indent=2)\n",
    "\n",
    "# ===============================\n",
    "# Agent Setup\n",
    "# ===============================\n",
    "\n",
    "def make_agent(remaining_sources):\n",
    "    return Agent(\n",
    "        task=(\n",
    "            \"Search for the latest investor research and commentary on momentum investing, \"\n",
    "            \"focusing on how strategies adapt to changing market dynamics. \"\n",
    "            \"Summarize key takeaways, including insights on portfolio construction, \"\n",
    "            \"trading costs, holding periods, scalability, and adaptation. \"\n",
    "            \"Return a clean markdown summary with sections: Key Findings, Implications for Investors, \"\n",
    "            \"and Practical Recommendations. \"\n",
    "            f\"Restrict yourself to these sources: {', '.join(remaining_sources)}.\"\n",
    "        ),\n",
    "        llm=llm,\n",
    "        browser_config={\n",
    "            \"headless\": True,\n",
    "            \"browser_type\": \"chromium\",\n",
    "            \"browser_timeout\": 60,\n",
    "            \"viewport_size\": {\"width\": 1280, \"height\": 720},\n",
    "            \"extra_chromium_args\": [\n",
    "                \"--no-sandbox\",\n",
    "                \"--disable-dev-shm-usage\",\n",
    "                \"--disable-gpu\",\n",
    "                \"--disable-extensions\",\n",
    "                \"--disable-plugins\",\n",
    "                \"--disable-images\",\n",
    "                \"--disable-javascript\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ===============================\n",
    "# Main Run (manual trigger)\n",
    "# ===============================\n",
    "\n",
    "async def run_research(batch_size=3):\n",
    "    visited = load_visited_sources()\n",
    "    remaining = get_remaining_sources()\n",
    "\n",
    "    if not remaining:\n",
    "        print(\"✅ All sources already covered.\")\n",
    "        return\n",
    "\n",
    "    next_batch = remaining[:batch_size]\n",
    "    print(f\"🔍 Running research on: {', '.join(next_batch)}\")\n",
    "\n",
    "    agent = make_agent(next_batch)\n",
    "\n",
    "    try:\n",
    "        history = await asyncio.wait_for(agent.run(max_steps=25), timeout=300)\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"❌ Agent timed out\")\n",
    "        return\n",
    "\n",
    "    final_result = history.final_result()\n",
    "    if final_result:\n",
    "        # Save markdown\n",
    "        append_markdown(f\"## Research Batch ({', '.join(next_batch)})\\n\\n{final_result}\")\n",
    "\n",
    "        # Save structured if available\n",
    "        if hasattr(history, \"structured_output\") and history.structured_output:\n",
    "            append_structured(history.structured_output)\n",
    "\n",
    "        print(f\"✅ Results from {next_batch} saved.\")\n",
    "    else:\n",
    "        print(\"⚠️ No final result produced.\")\n",
    "\n",
    "    # Update visited\n",
    "    visited.update(next_batch)\n",
    "    save_visited_sources(visited)\n",
    "\n",
    "# ===============================\n",
    "# Manual Execution\n",
    "# ===============================\n",
    "await run_research()\n",
    "# Example: run one batch\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🔧 Manual research agent ready.\")\n",
    "    print(\"➡️ Run: await run_research() inside Jupyter or asyncio.run(run_research()) in script\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc61a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing Enhanced Research Agent...\n",
      "✅ LLM initialized\n",
      "📊 Loaded 10 sources\n",
      "⚙️ Configuration loaded\n",
      "🚀 Enhanced Momentum Research Agent Ready!\n",
      "\n",
      "📋 QUICK START COMMANDS:\n",
      "   await run_research()                    # Run full pipeline (recommended)\n",
      "   await run_single_batch()               # Run just one batch quickly\n",
      "   await research_portfolio_construction() # Focus on portfolio construction\n",
      "   await research_trading_costs()          # Focus on trading costs\n",
      "   show_progress()                        # Check current progress\n",
      "   generate_quick_summary()               # View collected research\n",
      "   reset_progress()                       # Reset everything (careful!)\n",
      "\n",
      "🔧 CURRENT STATUS:\n",
      "\n",
      "📊 RESEARCH PROGRESS\n",
      "✅ Completed: 3/10 sources\n",
      "❌ Failed: 0 sources\n",
      "⏳ Remaining: 7 sources\n",
      "📝 Completed sources: investopedia.com, marketwatch.com, seekingalpha.com\n",
      "📄 Markdown file: 5,697 bytes\n",
      "\n",
      "⚙️ CONFIGURATION:\n",
      "   Session timeout: 120s\n",
      "   Max retries: 1\n",
      "   Min content length: 300 chars\n",
      "   Sources available: 10\n",
      "\n",
      "✅ Agent initialization complete - ready for research!\n"
     ]
    }
   ],
   "source": [
    "from browser_use import Agent, ChatGoogle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import hashlib\n",
    "\n",
    "# ===============================\n",
    "# Quick Setup with Immediate Feedback\n",
    "# ===============================\n",
    "\n",
    "print(\"🔧 Initializing Enhanced Research Agent...\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatGoogle(model=\"gemini-2.5-flash\")\n",
    "\n",
    "print(\"✅ LLM initialized\")\n",
    "\n",
    "# Streamlined source priorities\n",
    "HIGH_PRIORITY_SOURCES = [\"ssrn.com\", \"nber.org\", \"cfainstitute.org\", \"aqr.com\"]\n",
    "MEDIUM_PRIORITY_SOURCES = [\"morningstar.com\", \"seekingalpha.com\", \"investopedia.com\"]\n",
    "LOW_PRIORITY_SOURCES = [\"marketwatch.com\", \"finance.yahoo.com\", \"zacks.com\"]\n",
    "\n",
    "print(f\"📊 Loaded {len(HIGH_PRIORITY_SOURCES + MEDIUM_PRIORITY_SOURCES + LOW_PRIORITY_SOURCES)} sources\")\n",
    "\n",
    "# Reduced configuration for faster startup\n",
    "CONFIG = {\n",
    "    \"visited_sources_file\": \"visited_sources.json\",\n",
    "    \"markdown_path\": \"momentum_investing_summary.md\", \n",
    "    \"structured_path\": \"momentum_investing_data.json\",\n",
    "    \"failed_sources_file\": \"failed_sources.json\",\n",
    "    \"session_timeout\": 120,  # Reduced timeout\n",
    "    \"max_retries\": 1,        # Reduced retries for faster iteration\n",
    "    \"content_min_length\": 300,  # Reduced minimum for faster acceptance\n",
    "    \"cooldown_time\": 1       # Faster cooldown\n",
    "}\n",
    "\n",
    "print(\"⚙️ Configuration loaded\")\n",
    "\n",
    "# ===============================\n",
    "# Fast Helper Functions\n",
    "# ===============================\n",
    "\n",
    "def load_json_safe(filepath: str, default=None):\n",
    "    \"\"\"Fast JSON loading with minimal error handling\"\"\"\n",
    "    if default is None:\n",
    "        default = []\n",
    "    try:\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "    except:\n",
    "        pass\n",
    "    return default\n",
    "\n",
    "def save_json_safe(data, filepath: str):\n",
    "    \"\"\"Fast JSON saving\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Save error: {e}\")\n",
    "\n",
    "def get_visited_sources():\n",
    "    \"\"\"Get visited sources set\"\"\"\n",
    "    data = load_json_safe(CONFIG[\"visited_sources_file\"], [])\n",
    "    return set(data) if isinstance(data, list) else set()\n",
    "\n",
    "def save_visited_sources(visited: set):\n",
    "    \"\"\"Save visited sources\"\"\"\n",
    "    save_json_safe(list(visited), CONFIG[\"visited_sources_file\"])\n",
    "\n",
    "def get_failed_sources():\n",
    "    \"\"\"Get failed sources with timestamps\"\"\"\n",
    "    return load_json_safe(CONFIG[\"failed_sources_file\"], {})\n",
    "\n",
    "def save_failed_sources(failed: dict):\n",
    "    \"\"\"Save failed sources\"\"\"\n",
    "    save_json_safe(failed, CONFIG[\"failed_sources_file\"])\n",
    "\n",
    "def get_next_sources(batch_size: int = 2) -> List[str]:\n",
    "    \"\"\"Get next sources to process, prioritized and filtered\"\"\"\n",
    "    visited = get_visited_sources()\n",
    "    failed = get_failed_sources()\n",
    "    current_time = time.time()\n",
    "    \n",
    "    print(f\"📋 Already visited: {len(visited)} sources\")\n",
    "    print(f\"❌ Previously failed: {len(failed)} sources\")\n",
    "    \n",
    "    # Get available sources in priority order\n",
    "    available = []\n",
    "    \n",
    "    for tier, sources in [\n",
    "        (\"HIGH\", HIGH_PRIORITY_SOURCES),\n",
    "        (\"MEDIUM\", MEDIUM_PRIORITY_SOURCES), \n",
    "        (\"LOW\", LOW_PRIORITY_SOURCES)\n",
    "    ]:\n",
    "        for source in sources:\n",
    "            if source in visited:\n",
    "                continue\n",
    "            if source in failed:\n",
    "                # Allow retry after 30 minutes instead of 1 hour\n",
    "                if current_time - failed[source].get('timestamp', 0) < 1800:\n",
    "                    continue\n",
    "            available.append((source, tier))\n",
    "    \n",
    "    print(f\"🎯 Available sources: {len(available)}\")\n",
    "    \n",
    "    if available:\n",
    "        next_batch = [source for source, tier in available[:batch_size]]\n",
    "        print(f\"➡️ Next batch: {next_batch}\")\n",
    "        return next_batch\n",
    "    else:\n",
    "        print(\"✅ No more sources available\")\n",
    "        return []\n",
    "\n",
    "def save_results(content: str, sources: List[str], attempt: int = 1):\n",
    "    \"\"\"Quick result saving\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Save markdown\n",
    "    header = f\"\\n\\n---\\n## Research Results ({', '.join(sources)})\\n**Generated:** {timestamp} | **Attempt:** {attempt}\\n\\n\"\n",
    "    \n",
    "    try:\n",
    "        with open(CONFIG[\"markdown_path\"], \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(header + content)\n",
    "        print(f\"💾 Saved {len(content)} chars to markdown\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Markdown save error: {e}\")\n",
    "    \n",
    "    # Save structured\n",
    "    structured_data = {\n",
    "        \"sources\": sources,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"attempt\": attempt,\n",
    "        \"length\": len(content)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        existing = load_json_safe(CONFIG[\"structured_path\"], [])\n",
    "        existing.append(structured_data)\n",
    "        save_json_safe(existing, CONFIG[\"structured_path\"])\n",
    "        print(f\"💾 Saved structured data (total entries: {len(existing)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Structured save error: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# Fast Agent Creation\n",
    "# ===============================\n",
    "\n",
    "def create_fast_agent(sources: List[str]) -> Agent:\n",
    "    \"\"\"Create agent optimized for speed and reliability\"\"\"\n",
    "    \n",
    "    task = f\"\"\"\n",
    "    URGENT RESEARCH TASK - Focus on efficiency and results.\n",
    "    \n",
    "    Search for momentum investing research on these sources: {', '.join(sources)}\n",
    "    \n",
    "    FIND INFORMATION ON:\n",
    "    1. Portfolio construction for momentum strategies\n",
    "    2. Trading costs and implementation challenges  \n",
    "    3. Optimal holding periods\n",
    "    4. Scalability for different investor types\n",
    "    5. Strategy adaptation methods\n",
    "    \n",
    "    SEARCH TERMS TO USE:\n",
    "    - \"momentum investing strategies\"\n",
    "    - \"factor investing momentum\" \n",
    "    - \"quantitative momentum\"\n",
    "    - \"momentum portfolio construction\"\n",
    "    \n",
    "    OUTPUT FORMAT:\n",
    "    # Key Research Findings\n",
    "    \n",
    "    ## Portfolio Construction\n",
    "    [Specific methods and approaches found]\n",
    "    \n",
    "    ## Trading Costs & Implementation \n",
    "    [Cost analysis and practical considerations]\n",
    "    \n",
    "    ## Holding Periods\n",
    "    [Optimal timing and duration insights]\n",
    "    \n",
    "    ## Scalability & Adaptation\n",
    "    [Solutions for different investor types and market conditions]\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Minimum 500 words of substantive content\n",
    "    - Focus on actionable insights over general commentary\n",
    "    - Include specific data/metrics when available\n",
    "    - Skip generic introductions - get straight to findings\n",
    "    \n",
    "    SOURCES: {', '.join(sources)}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Minimal browser config for maximum compatibility\n",
    "    browser_config = {\n",
    "        \"headless\": True,\n",
    "        \"browser_type\": \"chromium\", \n",
    "        \"browser_timeout\": CONFIG[\"session_timeout\"],\n",
    "        \"viewport_size\": {\"width\": 1280, \"height\": 720},\n",
    "        \"extra_chromium_args\": [\n",
    "            \"--no-sandbox\",\n",
    "            \"--disable-dev-shm-usage\", \n",
    "            \"--disable-gpu\",\n",
    "            \"--disable-extensions\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return Agent(task=task, llm=llm, browser_config=browser_config)\n",
    "\n",
    "# ===============================\n",
    "# Main Research Functions\n",
    "# ===============================\n",
    "\n",
    "async def run_single_batch(batch_size: int = 2) -> bool:\n",
    "    \"\"\"Run a single batch of research - fast and focused\"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 Starting single batch research (batch size: {batch_size})\")\n",
    "    \n",
    "    # Get next sources\n",
    "    sources = get_next_sources(batch_size)\n",
    "    if not sources:\n",
    "        print(\"✅ All sources completed or unavailable\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"🎯 Processing: {sources}\")\n",
    "    \n",
    "    # Create and run agent\n",
    "    agent = create_fast_agent(sources)\n",
    "    \n",
    "    try:\n",
    "        print(\"⏱️ Starting agent execution...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run with timeout\n",
    "        history = await asyncio.wait_for(\n",
    "            agent.run(max_steps=20), \n",
    "            timeout=CONFIG[\"session_timeout\"]\n",
    "        )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"⏱️ Agent completed in {execution_time:.1f}s\")\n",
    "        \n",
    "        # Get results\n",
    "        result = history.final_result()\n",
    "        \n",
    "        if result and len(result) >= CONFIG[\"content_min_length\"]:\n",
    "            print(f\"✅ Success! Content length: {len(result)} chars\")\n",
    "            \n",
    "            # Save results\n",
    "            save_results(result, sources)\n",
    "            \n",
    "            # Mark as visited\n",
    "            visited = get_visited_sources()\n",
    "            visited.update(sources)\n",
    "            save_visited_sources(visited)\n",
    "            \n",
    "            print(f\"📝 Updated visited sources: {len(visited)} total\")\n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(f\"⚠️ Insufficient content: {len(result) if result else 0} chars\")\n",
    "            \n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"❌ Timeout after {CONFIG['session_timeout']}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Agent error: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Mark sources as failed\n",
    "    failed = get_failed_sources()\n",
    "    for source in sources:\n",
    "        failed[source] = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"error\": \"execution_failed\"\n",
    "        }\n",
    "    save_failed_sources(failed)\n",
    "    \n",
    "    print(f\"❌ Batch failed, marked sources as failed\")\n",
    "    return False\n",
    "\n",
    "async def run_research(max_batches: int = 5, batch_size: int = 2) -> Dict:\n",
    "    \"\"\"Run multiple research batches with progress tracking\"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 STARTING RESEARCH PIPELINE\")\n",
    "    print(f\"📊 Max batches: {max_batches}, Batch size: {batch_size}\")\n",
    "    \n",
    "    results = {\n",
    "        \"successful_batches\": 0,\n",
    "        \"failed_batches\": 0,\n",
    "        \"total_sources_processed\": 0,\n",
    "        \"start_time\": time.time()\n",
    "    }\n",
    "    \n",
    "    for batch_num in range(1, max_batches + 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"📋 BATCH {batch_num}/{max_batches}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        success = await run_single_batch(batch_size)\n",
    "        \n",
    "        if success:\n",
    "            results[\"successful_batches\"] += 1\n",
    "            results[\"total_sources_processed\"] += batch_size\n",
    "            print(f\"✅ Batch {batch_num} completed successfully\")\n",
    "        else:\n",
    "            results[\"failed_batches\"] += 1\n",
    "            print(f\"❌ Batch {batch_num} failed\")\n",
    "            \n",
    "            # Check if we should continue\n",
    "            remaining_sources = get_next_sources(batch_size)\n",
    "            if not remaining_sources:\n",
    "                print(\"🏁 No more sources available, stopping early\")\n",
    "                break\n",
    "        \n",
    "        # Progress summary\n",
    "        elapsed = time.time() - results[\"start_time\"]\n",
    "        print(f\"📊 Progress: {results['successful_batches']} success, {results['failed_batches']} failed, {elapsed:.1f}s elapsed\")\n",
    "        \n",
    "        # Brief cooldown between batches\n",
    "        if batch_num < max_batches:\n",
    "            print(\"⏸️ Brief cooldown...\")\n",
    "            await asyncio.sleep(CONFIG[\"cooldown_time\"])\n",
    "    \n",
    "    # Final summary\n",
    "    total_time = time.time() - results[\"start_time\"]\n",
    "    print(f\"\\n🏁 RESEARCH PIPELINE COMPLETED\")\n",
    "    print(f\"✅ Successful batches: {results['successful_batches']}\")\n",
    "    print(f\"❌ Failed batches: {results['failed_batches']}\")\n",
    "    print(f\"📊 Sources processed: {results['total_sources_processed']}\")\n",
    "    print(f\"⏱️ Total time: {total_time:.1f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ===============================\n",
    "# Focused Research Functions\n",
    "# ===============================\n",
    "\n",
    "async def research_portfolio_construction():\n",
    "    \"\"\"Quick focused research on portfolio construction\"\"\"\n",
    "    print(\"🎯 Focused research: Portfolio Construction\")\n",
    "    \n",
    "    sources = [\"aqr.com\", \"cfainstitute.org\"]\n",
    "    available_sources = [s for s in sources if s not in get_visited_sources()]\n",
    "    \n",
    "    if not available_sources:\n",
    "        available_sources = sources[:1]  # Force at least one\n",
    "    \n",
    "    agent = Agent(\n",
    "        task=f\"\"\"Research momentum investing portfolio construction methods from {available_sources}. \n",
    "        Focus on: security selection, position sizing, rebalancing frequency, risk management.\n",
    "        Provide specific methodologies and quantitative guidelines.\"\"\",\n",
    "        llm=llm\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        history = await asyncio.wait_for(agent.run(max_steps=15), timeout=90)\n",
    "        result = history.final_result()\n",
    "        \n",
    "        if result:\n",
    "            save_results(result, available_sources)\n",
    "            print(\"✅ Portfolio construction research completed\")\n",
    "            return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Portfolio research failed: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "async def research_trading_costs():\n",
    "    \"\"\"Quick focused research on trading costs\"\"\"\n",
    "    print(\"🎯 Focused research: Trading Costs\")\n",
    "    \n",
    "    sources = [\"investopedia.com\", \"morningstar.com\"] \n",
    "    available_sources = [s for s in sources if s not in get_visited_sources()]\n",
    "    \n",
    "    if not available_sources:\n",
    "        available_sources = sources[:1]\n",
    "    \n",
    "    agent = Agent(\n",
    "        task=f\"\"\"Research momentum investing trading costs and implementation from {available_sources}.\n",
    "        Focus on: transaction costs, market impact, bid-ask spreads, cost mitigation strategies.\n",
    "        Provide quantitative analysis and practical solutions.\"\"\",\n",
    "        llm=llm\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        history = await asyncio.wait_for(agent.run(max_steps=15), timeout=90)\n",
    "        result = history.final_result()\n",
    "        \n",
    "        if result:\n",
    "            save_results(result, available_sources)\n",
    "            print(\"✅ Trading costs research completed\")\n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Trading costs research failed: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ===============================\n",
    "# Utility Functions\n",
    "# ===============================\n",
    "\n",
    "def show_progress():\n",
    "    \"\"\"Show current research progress\"\"\"\n",
    "    visited = get_visited_sources()\n",
    "    failed = get_failed_sources()\n",
    "    all_sources = HIGH_PRIORITY_SOURCES + MEDIUM_PRIORITY_SOURCES + LOW_PRIORITY_SOURCES\n",
    "    \n",
    "    print(f\"\\n📊 RESEARCH PROGRESS\")\n",
    "    print(f\"✅ Completed: {len(visited)}/{len(all_sources)} sources\")\n",
    "    print(f\"❌ Failed: {len(failed)} sources\") \n",
    "    print(f\"⏳ Remaining: {len(all_sources) - len(visited)} sources\")\n",
    "    \n",
    "    if visited:\n",
    "        print(f\"📝 Completed sources: {', '.join(list(visited)[:5])}\" + (\"...\" if len(visited) > 5 else \"\"))\n",
    "    \n",
    "    # Check file sizes\n",
    "    try:\n",
    "        if os.path.exists(CONFIG[\"markdown_path\"]):\n",
    "            size = os.path.getsize(CONFIG[\"markdown_path\"])\n",
    "            print(f\"📄 Markdown file: {size:,} bytes\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def reset_progress():\n",
    "    \"\"\"Reset all progress - use carefully!\"\"\"\n",
    "    confirm = input(\"⚠️ Reset all progress? This will delete visited/failed source tracking. Type 'yes' to confirm: \")\n",
    "    if confirm.lower() == 'yes':\n",
    "        for file_path in [CONFIG[\"visited_sources_file\"], CONFIG[\"failed_sources_file\"]]:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "        print(\"✅ Progress reset completed\")\n",
    "    else:\n",
    "        print(\"❌ Reset cancelled\")\n",
    "\n",
    "def generate_quick_summary():\n",
    "    \"\"\"Generate a quick summary of collected research\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(CONFIG[\"markdown_path\"]):\n",
    "            with open(CONFIG[\"markdown_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            sections = content.split(\"---\")\n",
    "            print(f\"\\n📋 RESEARCH SUMMARY\")\n",
    "            print(f\"📄 Total sections: {len(sections)}\")\n",
    "            print(f\"📝 Total length: {len(content):,} characters\")\n",
    "            print(f\"💾 File: {CONFIG['markdown_path']}\")\n",
    "            \n",
    "            if len(sections) > 1:\n",
    "                print(\"✅ Research data collected successfully\")\n",
    "            else:\n",
    "                print(\"⚠️ Limited research data available\")\n",
    "        else:\n",
    "            print(\"❌ No research file found\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Summary generation error: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# Quick Start Interface\n",
    "# ===============================\n",
    "\n",
    "print(\"🚀 Enhanced Momentum Research Agent Ready!\")\n",
    "print(\"\\n📋 QUICK START COMMANDS:\")\n",
    "print(\"   await run_research()                    # Run full pipeline (recommended)\")\n",
    "print(\"   await run_single_batch()               # Run just one batch quickly\")\n",
    "print(\"   await research_portfolio_construction() # Focus on portfolio construction\")\n",
    "print(\"   await research_trading_costs()          # Focus on trading costs\")\n",
    "print(\"   show_progress()                        # Check current progress\")\n",
    "print(\"   generate_quick_summary()               # View collected research\")\n",
    "print(\"   reset_progress()                       # Reset everything (careful!)\")\n",
    "\n",
    "print(f\"\\n🔧 CURRENT STATUS:\")\n",
    "show_progress()\n",
    "\n",
    "print(f\"\\n⚙️ CONFIGURATION:\")\n",
    "print(f\"   Session timeout: {CONFIG['session_timeout']}s\") \n",
    "print(f\"   Max retries: {CONFIG['max_retries']}\")\n",
    "print(f\"   Min content length: {CONFIG['content_min_length']} chars\")\n",
    "print(f\"   Sources available: {len(HIGH_PRIORITY_SOURCES + MEDIUM_PRIORITY_SOURCES + LOW_PRIORITY_SOURCES)}\")\n",
    "\n",
    "print(f\"\\n✅ Agent initialization complete - ready for research!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68255745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5ed994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing Reliable Research Agent (Web Scraping Mode)...\n",
      "✅ Switching to reliable web scraping approach\n",
      "📊 Loaded 7 sources with direct URLs\n",
      "🚀 Reliable Research Agent Ready! (Direct Web Scraping)\n",
      "\n",
      "📋 QUICK START COMMANDS:\n",
      "   await run_research()                    # Full pipeline (reliable)\n",
      "   await run_single_batch()               # Single batch test\n",
      "   await research_portfolio_construction() # Focused portfolio research\n",
      "   await research_trading_costs()          # Focused trading costs\n",
      "   show_progress()                        # Check progress\n",
      "   generate_quick_summary()               # View results\n",
      "   reset_progress()                       # Reset (careful!)\n",
      "\n",
      "🔧 CURRENT STATUS:\n",
      "\n",
      "📊 RESEARCH PROGRESS\n",
      "✅ Completed: 3/7 sources\n",
      "❌ Failed: 0 sources\n",
      "⏳ Remaining: 4 sources\n",
      "📝 Completed sources: investopedia.com, marketwatch.com, seekingalpha.com\n",
      "📄 Markdown file: 5,697 bytes\n",
      "\n",
      "⚙️ CONFIGURATION:\n",
      "   Request timeout: 30s\n",
      "   Retry attempts: 2\n",
      "   Min content: 300 chars\n",
      "   Method: Direct web scraping (no browser automation)\n",
      "\n",
      "✅ Reliable agent ready - no CDP/browser issues!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ===============================\n",
    "# RELIABLE WEB SCRAPING APPROACH\n",
    "# ===============================\n",
    "\n",
    "print(\"🔧 Initializing Reliable Research Agent (Web Scraping Mode)...\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Switching to reliable web scraping approach\")\n",
    "\n",
    "# Source configurations with direct URLs\n",
    "RESEARCH_SOURCES = {\n",
    "    # High Priority - Academic/Research\n",
    "    \"ssrn.com\": {\n",
    "        \"search_urls\": [\n",
    "            \"https://papers.ssrn.com/sol3/results.cfm?requestsortorder=rank&submitresults=true&npage=1&perjury=n&form_name=journalBrowse&keywrd=momentum+investing\",\n",
    "            \"https://papers.ssrn.com/sol3/results.cfm?requestsortorder=rank&submitresults=true&npage=1&perjury=n&form_name=journalBrowse&keywrd=factor+investing+momentum\"\n",
    "        ],\n",
    "        \"priority\": 3\n",
    "    },\n",
    "    \"nber.org\": {\n",
    "        \"search_urls\": [\n",
    "            \"https://www.nber.org/papers?page=1&perpage=50&sortby=public_date&q=momentum+investing\",\n",
    "            \"https://www.nber.org/papers?page=1&perpage=50&sortby=public_date&q=factor+investing\"\n",
    "        ],\n",
    "        \"priority\": 3\n",
    "    },\n",
    "    \"cfainstitute.org\": {\n",
    "        \"search_urls\": [\n",
    "            \"https://rpc.cfainstitute.org/en/research-foundation/publications\",\n",
    "            \"https://rpc.cfainstitute.org/en/research-foundation/literature-reviews\"\n",
    "        ],\n",
    "        \"priority\": 3\n",
    "    },\n",
    "    \n",
    "    # Medium Priority - Professional\n",
    "    \"morningstar.com\": {\n",
    "        \"search_urls\": [\n",
    "            \"https://www.morningstar.com/search?q=momentum+investing\",\n",
    "            \"https://www.morningstar.com/search?q=factor+investing\"\n",
    "        ],\n",
    "        \"priority\": 2\n",
    "    },\n",
    "    \"seekingalpha.com\": {\n",
    "        \"search_urls\": [\n",
    "            \"https://seekingalpha.com/search?q=momentum+investing&tab=articles\",\n",
    "            \"https://seekingalpha.com/search?q=quantitative+momentum&tab=articles\"\n",
    "        ],\n",
    "        \"priority\": 2\n",
    "    },\n",
    "    \"investopedia.com\": {\n",
    "        \"search_urls\": [\n",
    "            \"https://www.investopedia.com/search?q=momentum+investing\",\n",
    "            \"https://www.investopedia.com/search?q=factor+investing\"\n",
    "        ],\n",
    "        \"priority\": 2\n",
    "    },\n",
    "    \n",
    "    # Lower Priority - General Financial Media\n",
    "    \"marketwatch.com\": {\n",
    "        \"search_urls\": [\n",
    "            \"https://www.marketwatch.com/search?q=momentum+investing\",\n",
    "            \"https://www.marketwatch.com/search?q=quantitative+strategies\"\n",
    "        ],\n",
    "        \"priority\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"visited_sources_file\": \"visited_sources.json\",\n",
    "    \"markdown_path\": \"momentum_investing_summary.md\", \n",
    "    \"structured_path\": \"momentum_investing_data.json\",\n",
    "    \"failed_sources_file\": \"failed_sources.json\",\n",
    "    \"request_timeout\": 30,\n",
    "    \"retry_attempts\": 2,\n",
    "    \"content_min_length\": 300,\n",
    "    \"cooldown_time\": 2,\n",
    "    \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "print(f\"📊 Loaded {len(RESEARCH_SOURCES)} sources with direct URLs\")\n",
    "\n",
    "# ===============================\n",
    "# UTILITY FUNCTIONS\n",
    "# ===============================\n",
    "\n",
    "def load_json_safe(filepath: str, default=None):\n",
    "    if default is None:\n",
    "        default = []\n",
    "    try:\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "    except:\n",
    "        pass\n",
    "    return default\n",
    "\n",
    "def save_json_safe(data, filepath: str):\n",
    "    try:\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Save error: {e}\")\n",
    "\n",
    "def get_visited_sources():\n",
    "    data = load_json_safe(CONFIG[\"visited_sources_file\"], [])\n",
    "    return set(data) if isinstance(data, list) else set()\n",
    "\n",
    "def save_visited_sources(visited: set):\n",
    "    save_json_safe(list(visited), CONFIG[\"visited_sources_file\"])\n",
    "\n",
    "def get_failed_sources():\n",
    "    return load_json_safe(CONFIG[\"failed_sources_file\"], {})\n",
    "\n",
    "def save_failed_sources(failed: dict):\n",
    "    save_json_safe(failed, CONFIG[\"failed_sources_file\"])\n",
    "\n",
    "# ===============================\n",
    "# WEB SCRAPING FUNCTIONS\n",
    "# ===============================\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': CONFIG[\"user_agent\"],\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "        })\n",
    "    \n",
    "    def fetch_content(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Fetch content from a URL with error handling\"\"\"\n",
    "        try:\n",
    "            print(f\"🌐 Fetching: {url}\")\n",
    "            response = self.session.get(url, timeout=CONFIG[\"request_timeout\"])\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            content = response.text\n",
    "            print(f\"✅ Fetched {len(content)} characters\")\n",
    "            return content\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"⏱️ Timeout fetching {url}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ Error fetching {url}: {str(e)[:100]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Unexpected error fetching {url}: {str(e)[:100]}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_research_content(self, html_content: str, source_name: str) -> str:\n",
    "        \"\"\"Extract relevant research content from HTML\"\"\"\n",
    "        if not html_content:\n",
    "            return \"\"\n",
    "        \n",
    "        # Simple text extraction - look for research-related content\n",
    "        content_lower = html_content.lower()\n",
    "        \n",
    "        # Find sections that likely contain research content\n",
    "        research_keywords = [\n",
    "            'momentum', 'portfolio', 'trading', 'investment', 'strategy',\n",
    "            'research', 'analysis', 'factor', 'quantitative', 'returns'\n",
    "        ]\n",
    "        \n",
    "        # Extract text around research keywords\n",
    "        relevant_sections = []\n",
    "        \n",
    "        for keyword in research_keywords:\n",
    "            start_pos = content_lower.find(keyword)\n",
    "            if start_pos != -1:\n",
    "                # Extract context around the keyword\n",
    "                context_start = max(0, start_pos - 200)\n",
    "                context_end = min(len(html_content), start_pos + 800)\n",
    "                context = html_content[context_start:context_end]\n",
    "                \n",
    "                # Clean up HTML tags (basic)\n",
    "                import re\n",
    "                clean_context = re.sub(r'<[^>]+>', ' ', context)\n",
    "                clean_context = re.sub(r'\\s+', ' ', clean_context).strip()\n",
    "                \n",
    "                if len(clean_context) > 100 and clean_context not in relevant_sections:\n",
    "                    relevant_sections.append(clean_context)\n",
    "        \n",
    "        # Combine sections\n",
    "        combined_content = '\\n\\n'.join(relevant_sections[:5])  # Limit to top 5 sections\n",
    "        \n",
    "        if len(combined_content) < CONFIG[\"content_min_length\"]:\n",
    "            # Fallback: extract any substantial text content\n",
    "            import re\n",
    "            text_content = re.sub(r'<[^>]+>', ' ', html_content)\n",
    "            text_content = re.sub(r'\\s+', ' ', text_content).strip()\n",
    "            \n",
    "            # Take first substantial chunk\n",
    "            if len(text_content) > CONFIG[\"content_min_length\"]:\n",
    "                combined_content = text_content[:2000]  # Limit to reasonable size\n",
    "        \n",
    "        return combined_content\n",
    "\n",
    "# ===============================\n",
    "# RESEARCH FUNCTIONS\n",
    "# ===============================\n",
    "\n",
    "def get_next_sources(batch_size: int = 2) -> List[str]:\n",
    "    \"\"\"Get next sources to process\"\"\"\n",
    "    visited = get_visited_sources()\n",
    "    failed = get_failed_sources()\n",
    "    current_time = time.time()\n",
    "    \n",
    "    print(f\"📋 Already visited: {len(visited)} sources\")\n",
    "    print(f\"❌ Previously failed: {len(failed)} sources\")\n",
    "    \n",
    "    # Get available sources by priority\n",
    "    available = []\n",
    "    \n",
    "    for source_name, config in RESEARCH_SOURCES.items():\n",
    "        if source_name in visited:\n",
    "            continue\n",
    "        if source_name in failed:\n",
    "            # Allow retry after 1 hour\n",
    "            if current_time - failed[source_name].get('timestamp', 0) < 3600:\n",
    "                continue\n",
    "        \n",
    "        available.append((source_name, config['priority']))\n",
    "    \n",
    "    # Sort by priority (highest first)\n",
    "    available.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    next_batch = [source for source, _ in available[:batch_size]]\n",
    "    print(f\"🎯 Available sources: {len(available)}\")\n",
    "    print(f\"➡️ Next batch: {next_batch}\")\n",
    "    \n",
    "    return next_batch\n",
    "\n",
    "def save_results(content: str, sources: List[str], method: str = \"web_scraping\"):\n",
    "    \"\"\"Save research results\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Save markdown\n",
    "    header = f\"\\n\\n---\\n## Research Results ({', '.join(sources)})\\n**Generated:** {timestamp} | **Method:** {method}\\n\\n\"\n",
    "    \n",
    "    try:\n",
    "        with open(CONFIG[\"markdown_path\"], \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(header + content)\n",
    "        print(f\"💾 Saved {len(content)} chars to markdown\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Markdown save error: {e}\")\n",
    "    \n",
    "    # Save structured\n",
    "    structured_data = {\n",
    "        \"sources\": sources,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"method\": method,\n",
    "        \"length\": len(content)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        existing = load_json_safe(CONFIG[\"structured_path\"], [])\n",
    "        existing.append(structured_data)\n",
    "        save_json_safe(existing, CONFIG[\"structured_path\"])\n",
    "        print(f\"💾 Saved structured data (total entries: {len(existing)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Structured save error: {e}\")\n",
    "\n",
    "async def research_source_direct(source_name: str) -> Optional[str]:\n",
    "    \"\"\"Research a single source using direct web scraping\"\"\"\n",
    "    print(f\"\\n🔍 Researching source: {source_name}\")\n",
    "    \n",
    "    if source_name not in RESEARCH_SOURCES:\n",
    "        print(f\"❌ Unknown source: {source_name}\")\n",
    "        return None\n",
    "    \n",
    "    source_config = RESEARCH_SOURCES[source_name]\n",
    "    scraper = WebScraper()\n",
    "    \n",
    "    all_content = []\n",
    "    \n",
    "    for url in source_config[\"search_urls\"]:\n",
    "        print(f\"📡 Fetching URL: {url}\")\n",
    "        \n",
    "        html_content = scraper.fetch_content(url)\n",
    "        if html_content:\n",
    "            research_content = scraper.extract_research_content(html_content, source_name)\n",
    "            if research_content:\n",
    "                all_content.append(research_content)\n",
    "                print(f\"✅ Extracted {len(research_content)} chars from {url}\")\n",
    "        \n",
    "        # Small delay between requests\n",
    "        await asyncio.sleep(1)\n",
    "    \n",
    "    if all_content:\n",
    "        # Combine all content from this source\n",
    "        combined = f\"\\n\\n## Research from {source_name}\\n\\n\" + \"\\n\\n### Section\\n\\n\".join(all_content)\n",
    "        \n",
    "        # Add analysis prompt for the LLM\n",
    "        analysis_prompt = f\"\"\"\n",
    "# Momentum Investing Research Analysis - {source_name}\n",
    "\n",
    "Based on the content gathered from {source_name}, here are the key insights:\n",
    "\n",
    "## Raw Research Content:\n",
    "{combined}\n",
    "\n",
    "## Key Findings:\n",
    "[The above content contains information about momentum investing strategies, portfolio construction, trading costs, and implementation approaches from {source_name}]\n",
    "\n",
    "## Actionable Insights:\n",
    "- Portfolio construction methodologies found\n",
    "- Trading cost considerations identified  \n",
    "- Implementation strategies discovered\n",
    "- Market adaptation approaches noted\n",
    "\n",
    "*Source: {source_name} | Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "        \n",
    "        return analysis_prompt\n",
    "    \n",
    "    print(f\"❌ No content extracted from {source_name}\")\n",
    "    return None\n",
    "\n",
    "async def run_single_batch(batch_size: int = 2) -> bool:\n",
    "    \"\"\"Run a single batch using web scraping\"\"\"\n",
    "    print(f\"\\n🚀 Starting single batch research (web scraping mode)\")\n",
    "    \n",
    "    sources = get_next_sources(batch_size)\n",
    "    if not sources:\n",
    "        print(\"✅ All sources completed or unavailable\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"🎯 Processing: {sources}\")\n",
    "    \n",
    "    successful_sources = []\n",
    "    all_research_content = []\n",
    "    \n",
    "    for source in sources:\n",
    "        try:\n",
    "            content = await research_source_direct(source)\n",
    "            if content and len(content) >= CONFIG[\"content_min_length\"]:\n",
    "                all_research_content.append(content)\n",
    "                successful_sources.append(source)\n",
    "                print(f\"✅ Successfully researched: {source}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Insufficient content from: {source}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error researching {source}: {e}\")\n",
    "    \n",
    "    if successful_sources and all_research_content:\n",
    "        # Combine all research\n",
    "        combined_research = \"\\n\\n\" + \"=\"*60 + \"\\n\\n\".join(all_research_content)\n",
    "        \n",
    "        # Save results\n",
    "        save_results(combined_research, successful_sources, \"direct_scraping\")\n",
    "        \n",
    "        # Mark as visited\n",
    "        visited = get_visited_sources()\n",
    "        visited.update(successful_sources)\n",
    "        save_visited_sources(visited)\n",
    "        \n",
    "        print(f\"✅ Batch completed: {len(successful_sources)} successful sources\")\n",
    "        return True\n",
    "    else:\n",
    "        # Mark all sources as failed\n",
    "        failed = get_failed_sources()\n",
    "        for source in sources:\n",
    "            failed[source] = {\n",
    "                \"timestamp\": time.time(),\n",
    "                \"error\": \"content_extraction_failed\",\n",
    "                \"method\": \"direct_scraping\"\n",
    "            }\n",
    "        save_failed_sources(failed)\n",
    "        \n",
    "        print(f\"❌ Batch failed: no content extracted\")\n",
    "        return False\n",
    "\n",
    "async def run_research(max_batches: int = 5, batch_size: int = 2) -> Dict:\n",
    "    \"\"\"Run multiple research batches\"\"\"\n",
    "    print(f\"\\n🚀 STARTING DIRECT SCRAPING RESEARCH PIPELINE\")\n",
    "    print(f\"📊 Max batches: {max_batches}, Batch size: {batch_size}\")\n",
    "    \n",
    "    results = {\n",
    "        \"successful_batches\": 0,\n",
    "        \"failed_batches\": 0,\n",
    "        \"total_sources_processed\": 0,\n",
    "        \"start_time\": time.time()\n",
    "    }\n",
    "    \n",
    "    for batch_num in range(1, max_batches + 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"📋 BATCH {batch_num}/{max_batches}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        success = await run_single_batch(batch_size)\n",
    "        \n",
    "        if success:\n",
    "            results[\"successful_batches\"] += 1\n",
    "            results[\"total_sources_processed\"] += batch_size\n",
    "            print(f\"✅ Batch {batch_num} completed successfully\")\n",
    "        else:\n",
    "            results[\"failed_batches\"] += 1\n",
    "            print(f\"❌ Batch {batch_num} failed\")\n",
    "            \n",
    "            # Check if we should continue\n",
    "            remaining_sources = get_next_sources(batch_size)\n",
    "            if not remaining_sources:\n",
    "                print(\"🏁 No more sources available, stopping early\")\n",
    "                break\n",
    "        \n",
    "        # Progress summary\n",
    "        elapsed = time.time() - results[\"start_time\"]\n",
    "        print(f\"📊 Progress: {results['successful_batches']} success, {results['failed_batches']} failed, {elapsed:.1f}s elapsed\")\n",
    "        \n",
    "        # Cooldown between batches\n",
    "        if batch_num < max_batches:\n",
    "            print(f\"⏸️ Cooldown {CONFIG['cooldown_time']}s...\")\n",
    "            await asyncio.sleep(CONFIG[\"cooldown_time\"])\n",
    "    \n",
    "    # Final summary\n",
    "    total_time = time.time() - results[\"start_time\"]\n",
    "    print(f\"\\n🏁 RESEARCH PIPELINE COMPLETED\")\n",
    "    print(f\"✅ Successful batches: {results['successful_batches']}\")\n",
    "    print(f\"❌ Failed batches: {results['failed_batches']}\")\n",
    "    print(f\"📊 Sources processed: {results['total_sources_processed']}\")\n",
    "    print(f\"⏱️ Total time: {total_time:.1f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ===============================\n",
    "# FOCUSED RESEARCH\n",
    "# ===============================\n",
    "\n",
    "async def research_portfolio_construction():\n",
    "    \"\"\"Focused research on portfolio construction\"\"\"\n",
    "    print(\"🎯 Focused research: Portfolio Construction\")\n",
    "    \n",
    "    # Use high-priority academic sources\n",
    "    target_sources = [\"cfainstitute.org\", \"ssrn.com\"]\n",
    "    available_sources = [s for s in target_sources if s not in get_visited_sources()]\n",
    "    \n",
    "    if not available_sources:\n",
    "        available_sources = target_sources[:1]  # Force research\n",
    "    \n",
    "    research_results = []\n",
    "    for source in available_sources[:1]:  # Limit to 1 for focused research\n",
    "        content = await research_source_direct(source)\n",
    "        if content:\n",
    "            research_results.append(content)\n",
    "    \n",
    "    if research_results:\n",
    "        combined = \"\\n\\n\".join(research_results)\n",
    "        save_results(combined, available_sources, \"focused_portfolio_construction\")\n",
    "        print(\"✅ Portfolio construction research completed\")\n",
    "        return combined\n",
    "    \n",
    "    print(\"❌ Portfolio construction research failed\")\n",
    "    return None\n",
    "\n",
    "async def research_trading_costs():\n",
    "    \"\"\"Focused research on trading costs\"\"\"\n",
    "    print(\"🎯 Focused research: Trading Costs\")\n",
    "    \n",
    "    target_sources = [\"investopedia.com\", \"morningstar.com\"]\n",
    "    available_sources = [s for s in target_sources if s not in get_visited_sources()]\n",
    "    \n",
    "    if not available_sources:\n",
    "        available_sources = target_sources[:1]\n",
    "    \n",
    "    research_results = []\n",
    "    for source in available_sources[:1]:\n",
    "        content = await research_source_direct(source)\n",
    "        if content:\n",
    "            research_results.append(content)\n",
    "    \n",
    "    if research_results:\n",
    "        combined = \"\\n\\n\".join(research_results)\n",
    "        save_results(combined, available_sources, \"focused_trading_costs\")\n",
    "        print(\"✅ Trading costs research completed\")\n",
    "        return combined\n",
    "    \n",
    "    print(\"❌ Trading costs research failed\")\n",
    "    return None\n",
    "\n",
    "# ===============================\n",
    "# UTILITY FUNCTIONS\n",
    "# ===============================\n",
    "\n",
    "def show_progress():\n",
    "    \"\"\"Show current research progress\"\"\"\n",
    "    visited = get_visited_sources()\n",
    "    failed = get_failed_sources()\n",
    "    all_sources = list(RESEARCH_SOURCES.keys())\n",
    "    \n",
    "    print(f\"\\n📊 RESEARCH PROGRESS\")\n",
    "    print(f\"✅ Completed: {len(visited)}/{len(all_sources)} sources\")\n",
    "    print(f\"❌ Failed: {len(failed)} sources\") \n",
    "    print(f\"⏳ Remaining: {len(all_sources) - len(visited)} sources\")\n",
    "    \n",
    "    if visited:\n",
    "        print(f\"📝 Completed sources: {', '.join(list(visited))}\")\n",
    "    \n",
    "    # Check file sizes\n",
    "    try:\n",
    "        if os.path.exists(CONFIG[\"markdown_path\"]):\n",
    "            size = os.path.getsize(CONFIG[\"markdown_path\"])\n",
    "            print(f\"📄 Markdown file: {size:,} bytes\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def reset_progress():\n",
    "    \"\"\"Reset all progress\"\"\"\n",
    "    confirm = input(\"⚠️ Reset all progress? Type 'yes' to confirm: \")\n",
    "    if confirm.lower() == 'yes':\n",
    "        for file_path in [CONFIG[\"visited_sources_file\"], CONFIG[\"failed_sources_file\"]]:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "        print(\"✅ Progress reset completed\")\n",
    "    else:\n",
    "        print(\"❌ Reset cancelled\")\n",
    "\n",
    "def generate_quick_summary():\n",
    "    \"\"\"Generate quick summary\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(CONFIG[\"markdown_path\"]):\n",
    "            with open(CONFIG[\"markdown_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            sections = content.split(\"---\")\n",
    "            print(f\"\\n📋 RESEARCH SUMMARY\")\n",
    "            print(f\"📄 Total sections: {len(sections)}\")\n",
    "            print(f\"📝 Total length: {len(content):,} characters\")\n",
    "            print(f\"💾 File: {CONFIG['markdown_path']}\")\n",
    "        else:\n",
    "            print(\"❌ No research file found\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Summary generation error: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# INITIALIZATION COMPLETE\n",
    "# ===============================\n",
    "\n",
    "print(\"🚀 Reliable Research Agent Ready! (Direct Web Scraping)\")\n",
    "print(\"\\n📋 QUICK START COMMANDS:\")\n",
    "print(\"   await run_research()                    # Full pipeline (reliable)\")\n",
    "print(\"   await run_single_batch()               # Single batch test\")\n",
    "print(\"   await research_portfolio_construction() # Focused portfolio research\")  \n",
    "print(\"   await research_trading_costs()          # Focused trading costs\")\n",
    "print(\"   show_progress()                        # Check progress\")\n",
    "print(\"   generate_quick_summary()               # View results\")\n",
    "print(\"   reset_progress()                       # Reset (careful!)\")\n",
    "\n",
    "print(f\"\\n🔧 CURRENT STATUS:\")\n",
    "show_progress()\n",
    "\n",
    "print(f\"\\n⚙️ CONFIGURATION:\")\n",
    "print(f\"   Request timeout: {CONFIG['request_timeout']}s\")\n",
    "print(f\"   Retry attempts: {CONFIG['retry_attempts']}\")\n",
    "print(f\"   Min content: {CONFIG['content_min_length']} chars\")\n",
    "print(f\"   Method: Direct web scraping (no browser automation)\")\n",
    "\n",
    "print(f\"\\n✅ Reliable agent ready - no CDP/browser issues!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f6ca44e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'browser_use.browser.browser'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbrowser_use\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbrowser_use\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbrowser\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbrowser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Browser\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbrowser_use\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbrowser\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BrowserContext\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'browser_use.browser.browser'"
     ]
    }
   ],
   "source": [
    "from browser_use import Agent\n",
    "from browser_use.browser.browser import Browser\n",
    "from browser_use.browser.context import BrowserContext\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# ===============================\n",
    "# Configuration & Setup\n",
    "# ===============================\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Latest browser_use compatible LLM setup\n",
    "try:\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.1)\n",
    "except ImportError:\n",
    "    from browser_use.llm.service import LLMService\n",
    "    llm = LLMService().get_llm(\"anthropic\")  # Fallback to Anthropic\n",
    "\n",
    "# Streamlined source configuration\n",
    "SOURCES = {\n",
    "    \"high\": [\"ssrn.com\", \"nber.org\", \"cfainstitute.org\", \"aqr.com\"],\n",
    "    \"medium\": [\"morningstar.com\", \"seekingalpha.com\", \"investopedia.com\"],\n",
    "    \"low\": [\"marketwatch.com\", \"finance.yahoo.com\", \"zacks.com\"]\n",
    "}\n",
    "\n",
    "CONFIG = {\n",
    "    \"visited_file\": \"visited.json\",\n",
    "    \"results_file\": \"momentum_research.md\",\n",
    "    \"timeout\": 90,\n",
    "    \"min_content\": 300,\n",
    "    \"max_steps\": 15\n",
    "}\n",
    "\n",
    "print(\"🚀 Browser_Use Research Agent Initialized\")\n",
    "\n",
    "# ===============================\n",
    "# Core Data Management\n",
    "# ===============================\n",
    "\n",
    "def load_data(file: str, default=None):\n",
    "    \"\"\"Load JSON data with error handling\"\"\"\n",
    "    try:\n",
    "        if Path(file).exists():\n",
    "            return json.loads(Path(file).read_text())\n",
    "    except Exception:\n",
    "        pass\n",
    "    return default or []\n",
    "\n",
    "def save_data(data, file: str):\n",
    "    \"\"\"Save data to JSON file\"\"\"\n",
    "    try:\n",
    "        Path(file).write_text(json.dumps(data, indent=2))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Save error: {e}\")\n",
    "\n",
    "def get_next_sources(count: int = 2) -> list:\n",
    "    \"\"\"Get next prioritized sources to process\"\"\"\n",
    "    visited = set(load_data(CONFIG[\"visited_file\"], []))\n",
    "    \n",
    "    # Flatten sources in priority order\n",
    "    all_sources = []\n",
    "    for tier in [\"high\", \"medium\", \"low\"]:\n",
    "        all_sources.extend(SOURCES[tier])\n",
    "    \n",
    "    available = [s for s in all_sources if s not in visited]\n",
    "    return available[:count]\n",
    "\n",
    "def mark_visited(sources: list):\n",
    "    \"\"\"Mark sources as visited\"\"\"\n",
    "    visited = set(load_data(CONFIG[\"visited_file\"], []))\n",
    "    visited.update(sources)\n",
    "    save_data(list(visited), CONFIG[\"visited_file\"])\n",
    "\n",
    "def save_results(content: str, sources: list):\n",
    "    \"\"\"Save research results\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    header = f\"\\n\\n---\\n## Research: {', '.join(sources)}\\n**Generated:** {timestamp}\\n\\n\"\n",
    "    \n",
    "    try:\n",
    "        with open(CONFIG[\"results_file\"], \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(header + content)\n",
    "        logger.info(f\"Saved {len(content)} chars from {sources}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Results save error: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# Browser_Use Agent Functions\n",
    "# ===============================\n",
    "\n",
    "async def create_browser_context() -> BrowserContext:\n",
    "    \"\"\"Create optimized browser context for latest browser_use\"\"\"\n",
    "    browser = Browser(\n",
    "        config={\n",
    "            \"headless\": True,\n",
    "            \"disable_security\": True,\n",
    "            \"chrome_instance_path\": None,  # Let browser_use auto-detect\n",
    "            \"keep_open\": False,\n",
    "            \"no_viewport\": False\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    context = await browser.new_context(\n",
    "        viewport={\"width\": 1280, \"height\": 720},\n",
    "        user_agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    \n",
    "    return context\n",
    "\n",
    "def create_research_task(sources: list) -> str:\n",
    "    \"\"\"Create focused research task prompt\"\"\"\n",
    "    return f\"\"\"\n",
    "Research momentum investing strategies from these sources: {', '.join(sources)}\n",
    "\n",
    "FOCUS AREAS:\n",
    "1. Portfolio construction methods and security selection\n",
    "2. Trading costs, implementation challenges, and solutions\n",
    "3. Optimal holding periods and rebalancing frequency\n",
    "4. Scalability for different investor types and AUM levels\n",
    "\n",
    "SEARCH STRATEGY:\n",
    "- Use terms: \"momentum investing\", \"factor investing\", \"quantitative momentum\"\n",
    "- Look for academic papers, research reports, and implementation guides\n",
    "- Focus on practical insights and quantitative metrics\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "## Key Findings from {', '.join(sources)}\n",
    "\n",
    "### Portfolio Construction\n",
    "[Specific methods found]\n",
    "\n",
    "### Implementation & Costs\n",
    "[Trading costs and practical considerations]\n",
    "\n",
    "### Timing & Rebalancing\n",
    "[Holding period insights]\n",
    "\n",
    "### Scalability Solutions\n",
    "[Adaptation strategies]\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Minimum 400 words substantive content\n",
    "- Include specific data/metrics when available\n",
    "- Focus on actionable insights\n",
    "- Skip generic introductions\n",
    "\"\"\"\n",
    "\n",
    "async def run_research_batch(sources: list) -> bool:\n",
    "    \"\"\"Execute research batch with latest browser_use patterns\"\"\"\n",
    "    if not sources:\n",
    "        return False\n",
    "    \n",
    "    logger.info(f\"Processing sources: {sources}\")\n",
    "    \n",
    "    try:\n",
    "        # Create browser context\n",
    "        context = await create_browser_context()\n",
    "        \n",
    "        # Create agent with latest browser_use API\n",
    "        agent = Agent(\n",
    "            task=create_research_task(sources),\n",
    "            llm=llm,\n",
    "            browser_context=context,\n",
    "            max_actions_per_step=3,  # Latest browser_use parameter\n",
    "            include_attributes=[\"text\", \"href\", \"title\"],  # Optimized for research\n",
    "        )\n",
    "        \n",
    "        # Execute with timeout\n",
    "        result = await asyncio.wait_for(\n",
    "            agent.run(max_steps=CONFIG[\"max_steps\"]),\n",
    "            timeout=CONFIG[\"timeout\"]\n",
    "        )\n",
    "        \n",
    "        # Process results\n",
    "        content = result.extract_content() if hasattr(result, 'extract_content') else str(result)\n",
    "        \n",
    "        if content and len(content) >= CONFIG[\"min_content\"]:\n",
    "            save_results(content, sources)\n",
    "            mark_visited(sources)\n",
    "            logger.info(f\"✅ Success: {len(content)} chars\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(f\"Insufficient content: {len(content) if content else 0} chars\")\n",
    "            \n",
    "    except asyncio.TimeoutError:\n",
    "        logger.error(f\"Timeout after {CONFIG['timeout']}s\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Research error: {str(e)[:100]}\")\n",
    "    finally:\n",
    "        try:\n",
    "            await context.close()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "# ===============================\n",
    "# Main Research Functions\n",
    "# ===============================\n",
    "\n",
    "async def run_research(max_batches: int = 5, batch_size: int = 2) -> dict:\n",
    "    \"\"\"Execute main research pipeline\"\"\"\n",
    "    logger.info(f\"🚀 Starting research pipeline: {max_batches} batches, size {batch_size}\")\n",
    "    \n",
    "    results = {\"successful\": 0, \"failed\": 0, \"start_time\": time.time()}\n",
    "    \n",
    "    for batch_num in range(1, max_batches + 1):\n",
    "        logger.info(f\"📋 Batch {batch_num}/{max_batches}\")\n",
    "        \n",
    "        sources = get_next_sources(batch_size)\n",
    "        if not sources:\n",
    "            logger.info(\"No more sources available\")\n",
    "            break\n",
    "        \n",
    "        success = await run_research_batch(sources)\n",
    "        \n",
    "        if success:\n",
    "            results[\"successful\"] += 1\n",
    "            logger.info(f\"✅ Batch {batch_num} completed\")\n",
    "        else:\n",
    "            results[\"failed\"] += 1\n",
    "            logger.info(f\"❌ Batch {batch_num} failed\")\n",
    "        \n",
    "        # Brief pause between batches\n",
    "        if batch_num < max_batches:\n",
    "            await asyncio.sleep(2)\n",
    "    \n",
    "    elapsed = time.time() - results[\"start_time\"]\n",
    "    logger.info(f\"🏁 Pipeline completed: {results['successful']} success, {results['failed']} failed in {elapsed:.1f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "async def quick_research(topic: str, source_count: int = 1) -> str:\n",
    "    \"\"\"Quick focused research on specific topic\"\"\"\n",
    "    sources = get_next_sources(source_count)\n",
    "    if not sources:\n",
    "        return \"No sources available\"\n",
    "    \n",
    "    logger.info(f\"Quick research: {topic} from {sources}\")\n",
    "    \n",
    "    try:\n",
    "        agent = Agent(\n",
    "            task=f\"Research {topic} in momentum investing from {sources[0]}. Provide specific, actionable insights in 200-400 words.\",\n",
    "            llm=llm,\n",
    "            browser_config=create_browser_config(),\n",
    "            use_vision=True\n",
    "        )\n",
    "        \n",
    "        history = await asyncio.wait_for(agent.run(max_steps=10), timeout=60)\n",
    "        result = history.final_result() if hasattr(history, 'final_result') else str(history)\n",
    "        \n",
    "        if result:\n",
    "            save_results(result, sources)\n",
    "            mark_visited(sources)\n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Quick research error: {e}\")\n",
    "    \n",
    "    return \"Research failed\"\n",
    "\n",
    "# ===============================\n",
    "# Utility Functions\n",
    "# ===============================\n",
    "\n",
    "def show_status():\n",
    "    \"\"\"Display current research status\"\"\"\n",
    "    visited = load_data(CONFIG[\"visited_file\"], [])\n",
    "    all_sources = sum(SOURCES.values(), [])\n",
    "    \n",
    "    print(f\"\\n📊 RESEARCH STATUS\")\n",
    "    print(f\"✅ Completed: {len(visited)}/{len(all_sources)} sources\")\n",
    "    print(f\"⏳ Remaining: {len(all_sources) - len(visited)}\")\n",
    "    \n",
    "    if Path(CONFIG[\"results_file\"]).exists():\n",
    "        size = Path(CONFIG[\"results_file\"]).stat().st_size\n",
    "        print(f\"📄 Results file: {size:,} bytes\")\n",
    "    \n",
    "    next_sources = get_next_sources(3)\n",
    "    if next_sources:\n",
    "        print(f\"➡️ Next: {', '.join(next_sources)}\")\n",
    "\n",
    "def reset_all():\n",
    "    \"\"\"Reset all progress\"\"\"\n",
    "    confirm = input(\"⚠️ Reset all progress? Type 'yes': \")\n",
    "    if confirm.lower() == 'yes':\n",
    "        for file in [CONFIG[\"visited_file\"]]:\n",
    "            Path(file).unlink(missing_ok=True)\n",
    "        print(\"✅ Progress reset\")\n",
    "    else:\n",
    "        print(\"❌ Cancelled\")\n",
    "\n",
    "# ===============================\n",
    "# Quick Start Interface\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n📋 QUICK START:\")\n",
    "print(\"   await run_research()                    # Full pipeline\")\n",
    "print(\"   await run_research_batch(['ssrn.com']) # Single batch\")\n",
    "print(\"   await quick_research('portfolio construction') # Quick topic research\")\n",
    "print(\"   show_status()                          # Check progress\")\n",
    "print(\"   reset_all()                            # Reset (careful!)\")\n",
    "\n",
    "show_status()\n",
    "print(\"\\n✅ Ready for research!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "browz1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
